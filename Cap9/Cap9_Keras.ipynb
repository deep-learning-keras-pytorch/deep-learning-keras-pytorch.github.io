{"cells":[{"cell_type":"markdown","metadata":{"id":"H6x6Q8DWqc0a"},"source":["# **Capítulo 9: Procesamiento de lenguaje natural**\n","\n","## Traducción automática de texto: de español a inglés"]},{"cell_type":"markdown","metadata":{"id":"S7JjoFCHqc0c"},"source":["Descargamos los datos del caso práctico"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":199,"status":"ok","timestamp":1690985542964,"user":{"displayName":"Carlos Garrido","userId":"04511245114116980775"},"user_tz":-120},"id":"t8IvToV9qc0d"},"outputs":[],"source":["!wget -q http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n","!unzip -q spa-eng.zip"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":65797,"status":"ok","timestamp":1690985608989,"user":{"displayName":"Carlos Garrido","userId":"04511245114116980775"},"user_tz":-120},"id":"X49qaXYeXhh0","outputId":"bcbba4b3-cafb-4749-f4eb-dd323151dd78"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m851.9/851.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for keras-nlp (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# !pip install keras_nlp\n","# !pip install keras-nlp --upgrade\n","# !pip install -q git+https://github.com/keras-team/keras-nlp.git --upgrade\n","!pip install -q git+https://github.com/keras-team/keras-nlp.git --upgrade"]},{"cell_type":"markdown","metadata":{"id":"KZ9CTBBbqc0e"},"source":["Carga del conjunto de datos de traducción automática"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":365,"status":"ok","timestamp":1690985609346,"user":{"displayName":"Carlos Garrido","userId":"04511245114116980775"},"user_tz":-120},"id":"5OcSu1zGqc0e","outputId":"c68031a5-af55-4842-d17b-f9ad49a4808b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Número de pares de oraciones: 118964\n","Posible entrada: Estoy levantado.\n","Posible salida: I'm up.\n"]}],"source":["def cargar_datos():\n","    with open('spa-eng/spa.txt', 'r') as f:\n","        lineas = f.read().splitlines()\n","    pares = [linea.split('\\t') for linea in lineas]\n","    esp = [par[1] for par in pares]\n","    ing = [par[0] for par in pares]\n","    return esp, ing\n","\n","X, Y = cargar_datos()\n","print(f'Número de pares de oraciones: {len(X)}')\n","print(f'Posible entrada: {X[50]}')\n","print(f'Posible salida: {Y[50]}')"]},{"cell_type":"markdown","metadata":{"id":"nNp97-kxqc0f"},"source":["Creación de los vocabularios de español e inglés"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1034,"status":"ok","timestamp":1690985610378,"user":{"displayName":"Carlos Garrido","userId":"04511245114116980775"},"user_tz":-120},"id":"u-D5tKiYqc0f","outputId":"aa8bf09d-784f-4ef5-f61c-28aa8fb8e1b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño del vocabulario de español: 28993\n","Tamaño del vocabulario de inglés: 14779\n"]}],"source":["import re\n","\n","def crear_vocab(frases):\n","    # Obtenemos el vocabulario\n","    vocab = set()\n","    for f in frases:\n","        # Expresión regular para separar palabras\n","        # manteniendo signos de puntuación\n","        vocab.update(re.findall(r'\\w+|[^\\w\\s]', f))\n","\n","    # Creamos los diccionarios\n","    w2i = {w: i+4 for i, w in enumerate(vocab)}\n","    w2i['PAD'] = 0\n","    w2i['SOS'] = 1\n","    w2i['EOS'] = 2\n","    w2i['UNK'] = 3\n","    i2w = {i: w for w, i in w2i.items()}\n","\n","    return w2i, i2w\n","\n","X_w2i, X_i2w = crear_vocab(X)\n","Y_w2i, Y_i2w = crear_vocab(Y)\n","print(f'Tamaño del vocabulario de español: {len(X_w2i)}')\n","print(f'Tamaño del vocabulario de inglés: {len(Y_w2i)}')"]},{"cell_type":"markdown","metadata":{"id":"NEJjeh_aqc0g"},"source":["Codificación de las secuencias"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1623,"status":"ok","timestamp":1690985611997,"user":{"displayName":"Carlos Garrido","userId":"04511245114116980775"},"user_tz":-120},"id":"Iu4FiMhwqc0g"},"outputs":[],"source":["def codificar(secs, w2i):\n","    secs_cod = []\n","    for s in secs:\n","        s_cod = [w2i[w] for w in re.findall(r'\\w+|[^\\w\\s]', s)]\n","        s_cod = [w2i['SOS']] + s_cod + [w2i['EOS']]\n","        secs_cod.append(s_cod)\n","    return secs_cod\n","\n","X_cod = codificar(X, X_w2i)\n","Y_cod = codificar(Y, Y_w2i)"]},{"cell_type":"markdown","metadata":{"id":"OrCs6ZxPqc0h"},"source":["División del conjunto de datos en entrenamiento y test (80-20)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":661,"status":"ok","timestamp":1690985612656,"user":{"displayName":"Carlos Garrido","userId":"04511245114116980775"},"user_tz":-120},"id":"hALis3qZqc0h","outputId":"4b86ccef-460c-4c7e-b036-fbe35027f4fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["¡Particiones realizadas!\n","Tamaño del conjunto de entrenamiento: 95171\n","Tamaño del conjunto de test: 23793\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X_cod, Y_cod,\\\n","                                                    test_size=0.2,\\\n","                                                    random_state=42)\n","print('¡Particiones realizadas!')\n","print(f'Tamaño del conjunto de entrenamiento: {len(X_train)}')\n","print(f'Tamaño del conjunto de test: {len(X_test)}')"]},{"cell_type":"markdown","metadata":{"id":"sAU5pnQzqc0h"},"source":["Preprocesado de los datos de entrenamiento"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1690985613146,"user":{"displayName":"Carlos Garrido","userId":"04511245114116980775"},"user_tz":-120},"id":"PbazgppAqc0h"},"outputs":[],"source":["import numpy as np\n","\n","def preproceso_batch(X, Y):\n","    max_long_X = max([len(x) for x in X])\n","    max_long_Y = max([len(y) for y in Y])\n","\n","    encoder_entrada = np.zeros((len(X), max_long_X))\n","    decoder_entrada = np.zeros((len(Y), max_long_Y))\n","    salida = np.zeros((len(Y), max_long_Y))\n","\n","    for i, s in enumerate(X):\n","        # Sec. completa con relleno para el encoder (frase a traducir)\n","        encoder_entrada[i, :len(s)] = np.array(s)\n","\n","    for i, s in enumerate(Y):\n","        # Sec. sin el \"EOS\" con relleno para el decoder (traducción)\n","        decoder_entrada[i, :len(s)-1] = np.array(s[:-1])\n","        # Sec. sin el \"SOS\" con relleno para la salida (traducción)\n","        salida[i, :len(s)-1] = np.array(s[1:])\n","\n","    encoder_entrada = encoder_entrada.astype(np.int64)\n","    decoder_entrada = decoder_entrada.astype(np.int64)\n","    salida = salida.astype(np.int64)\n","\n","\n","    return encoder_entrada, decoder_entrada, salida\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1690985613146,"user":{"displayName":"Carlos Garrido","userId":"04511245114116980775"},"user_tz":-120},"id":"9SpVpY0Gwqo2"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"kz_FKFRXqc0h"},"source":["Creación de un generador de batches o data loader"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1690985613147,"user":{"displayName":"Carlos Garrido","userId":"04511245114116980775"},"user_tz":-120},"id":"faJzMLscqc0h","outputId":"c8ea9c5a-5178-45c6-97ac-74b403ebc848"},"outputs":[{"output_type":"stream","name":"stdout","text":["Entrada al encoder: ['SOS', 'No', 'tengo', 'otra', 'opción', 'en', 'absoluto', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","Entrada al decoder: ['SOS', 'I', 'have', 'no', 'choice', 'at', 'all', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","Salida del decoder: ['I', 'have', 'no', 'choice', 'at', 'all', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n"]}],"source":["from sklearn.utils import shuffle\n","\n","def generador_batch(X, Y, batch_size):\n","    idx = 0\n","    while True:\n","        bx = X[idx:idx+batch_size]\n","        by = Y[idx:idx+batch_size]\n","\n","        yield preproceso_batch(bx, by)\n","\n","        idx = (idx + batch_size)\n","        if idx >= len(X):\n","            X, Y = shuffle(X, Y, random_state=42)\n","            return\n","\n","\n","tam_batch = 128\n","train_loader = generador_batch(X_train, Y_train, batch_size=tam_batch)\n","be, bd, bs = next(train_loader)\n","print(f'Entrada al encoder: {[X_i2w[w.item()]for w in be[0]]}')\n","print(f'Entrada al decoder: {[Y_i2w[w.item()]for w in bd[0]]}')\n","print(f'Salida del decoder: {[Y_i2w[w.item()]for w in bs[0]]}')"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1690985613148,"user":{"displayName":"Carlos Garrido","userId":"04511245114116980775"},"user_tz":-120},"id":"BjrRLdMalc_x"},"outputs":[],"source":["test_loader = generador_batch(X_test, Y_test, batch_size=tam_batch)"]},{"cell_type":"markdown","metadata":{"id":"Xc2_j1vZqc0i"},"source":["Definición de la capa de codificación de posición"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":7126,"status":"ok","timestamp":1690985620268,"user":{"displayName":"Carlos Garrido","userId":"04511245114116980775"},"user_tz":-120},"id":"AowCb_3Gqc0i"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","\n","class PositionalEncoding(tf.keras.layers.Layer):\n","    def __init__(self, max_len, emb_dim, dropout=0.1):\n","      super(PositionalEncoding, self).__init__()\n","      self.dropout = tf.keras.layers.Dropout(dropout)\n","\n","      pos = np.arange(max_len).reshape(-1, 1)\n","      den = np.power(10000, np.arange(0, emb_dim, 2) / emb_dim)\n","      pe = np.zeros((1, max_len, emb_dim))\n","      pe[0, :, 0::2] = np.sin(pos / den)\n","      pe[0, :, 1::2] = np.cos(pos / den)\n","      self.pe = tf.constant(pe, dtype=tf.float32)\n","\n","    def call(self, x):\n","      # x.shape = [batch_size, sec_len, emb_dim]\n","      x = x + self.pe[:, :tf.shape(x)[1], :]\n","      return self.dropout(x)\n"]},{"cell_type":"markdown","metadata":{"id":"LVxan_-dqc0i"},"source":["Definición del modelo Transformer a usar"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":732,"status":"ok","timestamp":1690985620996,"user":{"displayName":"Carlos Garrido","userId":"04511245114116980775"},"user_tz":-120},"id":"dv3ubR65TBzC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"461e56dd-278b-4b01-a30b-02f5c4b41678"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using TensorFlow backend\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","import keras_nlp\n","\n","# Define la clase Transformer\n","class Transformer(tf.keras.Model):\n","  def __init__(self,\n","                max_long,\n","                emb_dim,\n","                num_enc_capas,\n","                num_dec_capas,\n","                ncabezas,\n","                src_vocab_tam,\n","                tgt_vocab_tam,\n","                dim_mlp,\n","                dropout=0.1):\n","      super(Transformer, self).__init__()\n","\n","      # Capas de embedding + codificación de posición\n","      self.src_emb = tf.keras.layers.Embedding(src_vocab_tam, emb_dim)\n","      self.tgt_emb = tf.keras.layers.Embedding(tgt_vocab_tam, emb_dim)\n","\n","      enc_entradas = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"enc_entradas\")\n","      enc_salidas = self.src_emb(enc_entradas)\n","      enc_salidas = PositionalEncoding(max_long, emb_dim, 0.1)(enc_salidas)\n","\n","      # Encoder\n","      for i in range(num_enc_capas):\n","        enc_salidas = keras_nlp.layers.TransformerEncoder(\n","            intermediate_dim=dim_mlp,\n","            num_heads=ncabezas,\n","            dropout=dropout,\n","            activation=\"relu\",\n","            name=None,\n","          )(enc_salidas)\n","\n","      self.encoder = tf.keras.Model(enc_entradas, enc_salidas)\n","\n","      # Decoder\n","      dec_entradas = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"dec_entradas\")\n","      dec_salidas = self.tgt_emb(dec_entradas)\n","      dec_salidas = PositionalEncoding(max_long, emb_dim, 0.1)(dec_salidas)\n","\n","      enc_seq_entradas = keras.Input(shape=(None, emb_dim), name=\"dec_state_entradas\")\n","\n","      for _ in range(num_dec_capas):\n","        dec_salidas = keras_nlp.layers.TransformerDecoder(\n","            intermediate_dim=dim_mlp,\n","            num_heads=ncabezas,\n","            dropout=dropout,\n","            activation=\"relu\",\n","            name=None,\n","          )(decoder_sequence=dec_salidas, encoder_sequence=enc_seq_entradas)\n","\n","      dec_salidas = tf.keras.layers.Dense(tgt_vocab_tam, activation=\"linear\")(dec_salidas)\n","      self.decoder = tf.keras.Model([\n","              dec_entradas, # input 1\n","              enc_seq_entradas, # input 2\n","          ],\n","          dec_salidas, # output\n","      )\n","\n","      dec_salidas = self.decoder([dec_entradas, enc_salidas])\n","      self.transformer = tf.keras.Model(\n","        [enc_entradas, dec_entradas],\n","        dec_salidas,\n","        name=\"transformer\",\n","      )\n","\n","  def call(self, src, tgt):\n","      src_mask, _, _, _ = self.crear_mascara(src, tgt)\n","\n","      # Transformer Encoder\n","      memory = self.encoder(src, mask=src_mask, training=True)\n","\n","\n","      # Transformer Decoder\n","      tgt_pred = self.decoder([tgt, memory], training=True)\n","\n","      return tgt_pred\n","\n","  def codificar(self, src, src_mask):\n","      # Embedding + codificación de posición\n","      return self.encoder(src, mask=src_mask, training=False)\n","\n","  def decodificar(self, tgt, memory, tgt_mask):\n","      tgt_pred = self.decoder([tgt, memory], training=False)\n","      return tgt_pred\n","\n","  def crear_mascara(self, src, tgt):\n","      # src/tgt.shape = [batch_size, src/tgt_sec_len, emb_dim]\n","      src_sec_len = tf.shape(src)[1]\n","      tgt_sec_len = tf.shape(tgt)[1]\n","\n","      # Máscara de ceros (dejamos ver todo)\n","      src_mask = tf.zeros((src_sec_len, src_sec_len))\n","      # Máscara triangular superior para el target\n","      tgt_mask = tf.linalg.LinearOperatorLowerTriangular(tf.ones((tgt_sec_len, tgt_sec_len))).to_dense()\n","\n","      # 0 == \"PAD\"\n","      src_pad_mask = (src == 0)\n","      tgt_pad_mask = (tgt == 0)\n","\n","      return src_mask, tgt_mask, src_pad_mask, tgt_pad_mask\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1690985620998,"user":{"displayName":"Carlos Garrido","userId":"04511245114116980775"},"user_tz":-120},"id":"fO3SyH3pS_UN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e981ca31-ac33-45f8-e7bd-b88458fd78b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["278\n"]}],"source":["max_long = max([len(x) for x in X + Y])\n","print(max_long)"]},{"cell_type":"markdown","metadata":{"id":"Yb_HiGK1qc0i"},"source":["Instancia del modelo Transformer a usar"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":13958,"status":"ok","timestamp":1690985634951,"user":{"displayName":"Carlos Garrido","userId":"04511245114116980775"},"user_tz":-120},"id":"Z-OZIiHiwcep"},"outputs":[],"source":["# Instancia del modelo Transformer\n","modelo = Transformer(\n","    max_long=max_long,\n","    emb_dim=512,\n","    num_enc_capas=6,\n","    num_dec_capas=6,\n","    ncabezas=8,\n","    src_vocab_tam=len(X_w2i),\n","    tgt_vocab_tam=len(Y_w2i),\n","    dim_mlp=2048,\n","    dropout=0.1\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":432,"status":"ok","timestamp":1690985635377,"user":{"displayName":"Carlos Garrido","userId":"04511245114116980775"},"user_tz":-120},"id":"-ozmKC30csCO","outputId":"77b04e5c-47f2-48a0-d202-1c74cce63bc6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"transformer\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," enc_entradas (InputLayer)   [(None, None)]               0         []                            \n","                                                                                                  \n"," embedding (Embedding)       (None, None, 512)            1484441   ['enc_entradas[0][0]']        \n","                                                          6                                       \n","                                                                                                  \n"," positional_encoding (Posit  (None, None, 512)            0         ['embedding[0][0]']           \n"," ionalEncoding)                                                                                   \n","                                                                                                  \n"," transformer_encoder (Trans  (None, None, 512)            3152384   ['positional_encoding[0][0]'] \n"," formerEncoder)                                                                                   \n","                                                                                                  \n"," transformer_encoder_1 (Tra  (None, None, 512)            3152384   ['transformer_encoder[0][0]'] \n"," nsformerEncoder)                                                                                 \n","                                                                                                  \n"," transformer_encoder_2 (Tra  (None, None, 512)            3152384   ['transformer_encoder_1[0][0]'\n"," nsformerEncoder)                                                   ]                             \n","                                                                                                  \n"," transformer_encoder_3 (Tra  (None, None, 512)            3152384   ['transformer_encoder_2[0][0]'\n"," nsformerEncoder)                                                   ]                             \n","                                                                                                  \n"," transformer_encoder_4 (Tra  (None, None, 512)            3152384   ['transformer_encoder_3[0][0]'\n"," nsformerEncoder)                                                   ]                             \n","                                                                                                  \n"," dec_entradas (InputLayer)   [(None, None)]               0         []                            \n","                                                                                                  \n"," transformer_encoder_5 (Tra  (None, None, 512)            3152384   ['transformer_encoder_4[0][0]'\n"," nsformerEncoder)                                                   ]                             \n","                                                                                                  \n"," model_1 (Functional)        (None, None, 14779)          4037266   ['dec_entradas[0][0]',        \n","                                                          7          'transformer_encoder_5[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n","==================================================================================================\n","Total params: 74131387 (282.79 MB)\n","Trainable params: 74131387 (282.79 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n","None\n"]}],"source":["# print(modelo.transformer.summary())\n","print(modelo.transformer.summary())\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nxoOLk_xp4ds","outputId":"c932841f-36af-4049-a1b4-57290f00f1f6","executionInfo":{"status":"ok","timestamp":1690991935250,"user_tz":-120,"elapsed":5608280,"user":{"displayName":"Carlos Garrido","userId":"04511245114116980775"}}},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Comienzo de epoca 0\n","Prediccion:    [[ 4153 14272 11265 11149  8712  8123 12662   663 10446 10131   663 11391\n","   5555 11149  4264 12816 12816 12816 11149   604  9433]\n"," [ 8123 11149   149  9433   849  8529  4534  9433  8520 12252   663 11510\n","   5353  5353  4264 12816 12816 12816  9433 12816 13860]\n"," [ 6612  2908 11341 14272 14272  9433 13304   663  8520 13304   663 10006\n","   8388  2818  4264 12816 12816 12816 11149   604 11149]\n"," [ 3339  6700 14272 14272  8123  9042  2345  8520 13304   663  5016 11149\n","   5353 11149 11149 12816 12816  9032 12816  8123  7510]\n"," [11149  2908 11149  8712  3748  5353 11510  8073 13304 11831  5016 14272\n","   2908  9433 11831 12816 12816 12816  3087 13152  2345]]\n","Ground truth: [[ 7867  6458 13888 11436 13927 14455 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867 13366  9102 12853 14084 12166     2     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7804  6635  1682  2016     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [10661  5172 13366  2269   180  7396  5240  3017  4361 12166     2     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867 14422 10039  9102  6204  1687  3017  7323 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x78b50f98c310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x78b50f98c310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["Pérdida media en el step 0/743: 9.592031478881836\n","Prediccion:    [[ 7867  2194  2194 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166     2]\n"," [ 7867  2194  2194 12166 12166 12166 12166 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166     2 12166     2     2]\n"," [ 7867  2194  2194 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","      2 12166 12166 12166 12166 12166 12166     2     2]\n"," [ 7867  2194  2194 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194  2194 12166 12166 12166 12166 12166     2     2 12166 12166\n","  12166 12166 12166 12166 12166     2     2     2     2]]\n","Ground truth: [[ 3651 13390  7866  2016     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7526  2194  3712 12436  9102  3017  9092 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [  745  3271  7717 11059  4624 13615 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 3651 13941  6419     2     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [11229 14572 13307 12811  9102  9037  5771 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 100/743: 6.49453592300415\n","Prediccion:    [[ 7867  2194  2194 12166 12166 12166 12166     2     2 12166 12166 12166\n","  12166 12166 12166 12166 12166     2 12166 12166     2 12166 12166     2\n","  12166]\n"," [ 7867  2194  2194 12249 12249 12166 12166 12166 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166     2\n","  12166]\n"," [ 7867  2194  2194 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 7867  2194  2194  9102 12166 12166 12166 12166     2     2 12166 12166\n","  12166 12166 12166 12166 12166     2 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 7867  2194  2194 12249 12166 12166 12166 12166 12166 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]]\n","Ground truth: [[ 7867 11379  9793  1325  2269 14154 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  6472  2194 12249 14422  9102 12853  6010 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 7867  6390  7261 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  8410  2461  8285 10256  3017  2860 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664 11433 12217 11182  8888  9102  2295  2269  1648 12166     2     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]]\n","Pérdida media en el step 200/743: 5.769333362579346\n","Prediccion:    [[ 7867  2194  3017  3017 12166 12166 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 7867  2194  2194  9102 12166  3017 12166 12166 12166  2194 12166     2\n","      2 12166 12166 12166 12166]\n"," [ 7867  8410  8410 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 7867  8410  9102  3017 12166 12166  3017 12166     2     2 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 7867  2194  3017  3017 12166 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]]\n","Ground truth: [[ 7867 10932  2269 14156  4080  1945 12166     2     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7532  5122  9842  1687  3017  6297 12829  3520  9102  9104 12166     2\n","      0     0     0     0     0]\n"," [ 8573  2009 10559 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 9023  5418 13801  2724 11346 10256  7450 12166     2     0     0     0\n","      0     0     0     0     0]\n"," [10751 13366  2269 10609  3500 12166     2     0     0     0     0     0\n","      0     0     0     0     0]]\n","Pérdida media en el step 300/743: 5.425690650939941\n","Prediccion:    [[ 7867  2194  9102  7694  9102  3017  3017 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 7867 10039  2194  9102 12166 12166  3017  3017 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 7867 10039  8410 12166 12166  2194  3712  3017 12166  3017 12166 12166\n","   2194  2194 12249  2194 12166  7694 12166     2     2     2     2     2\n","  12166]\n"," [ 7867 10039  8410 12166  3017 12166  3017  3017 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 7867  2194  2269  2269 12166     2 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]]\n","Ground truth: [[ 9114 10142  9102 14407 11059  6635  4964 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 9120 10039  8679  5992 10941 13071  1639 10255  2016     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1862  8946  9302   925  7867  2194 11643  3961 13046  1430  8812  7867\n","    936  2194 12249  3362  2269  5654 12166     2     0     0     0     0\n","      0]\n"," [10661  7621 13017  4090  3862  5240  3017   977 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  8410  1861   873 12166     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]]\n","Pérdida media en el step 400/743: 5.198110103607178\n","Prediccion:    [[ 7867  2194  2194 12249 14535  9102  3017 10039  3017 12166 12166     2\n","  12166 12166 12166 12166 12166]\n"," [ 7867  8410  2269 12049  2269 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 7867 10039  2269  9102  3017 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 7867 10039 10039  7694  3017  2016     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 7867  2194 11643  7694  2269  9102  9102  7694 12049 12166  3017 12166\n","  12166 12166     2 12166 12166]]\n","Ground truth: [[ 7867  7637  2194 12249   394  7029 10039 13071  6010  3660 12166     2\n","      0     0     0     0     0]\n"," [ 1664 13390  2269 14156  6256 12166     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 5111  8410  8950 11059  1748 12166     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7804  1664 14124  2269  7546  2016     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7532  2194 12541  7694  6755  8046  9102  3017  9075 10256  3376 10039\n","  12853 12166     2     0     0]]\n","Pérdida media en el step 500/743: 5.033185005187988\n","Prediccion:    [[ 7867  8410  2194 10039  9102  3017  2016 12049  2016  2016 12166     2\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194 10039 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194  9102  2269 12166 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867 10039  2194 10039  2194     2 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  8410  3017 12049 12166  3017 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[14286 10039 14134   925  9102   219  3017  9655  1325 10039  2016     2\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867  9266  8888 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 1166  8612   124 14084 13390 12166     2     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 2981  7867 10743 10039  2016     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [11229  8492  3017  1932 11059  4571 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 600/743: 4.898561000823975\n","Prediccion:    [[ 7867  8410  2194 12249 14535  9102  2269 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194  3712 10039 10039  3017 10039 10039 12049 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166     2     2 12166     2 12166 12166]\n"," [ 7867  2194  2269  2269 12049 12166 12166  3017 12166 12166 12166     2\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194  9102  9102  3017 14404 10102 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194  2269 12049  9102     2 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[  121  6472  2194 12249 12200 11059  9409 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7526  2194  3712 10047  2330  4029   867  2269 14484 12166     2     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7867  6458  2600  2269  3049 14329 11059   835 10256   925 12166     2\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [  121  3042  2009  6041 13999  3017   705 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7867 13280  8729  6855 12166     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 700/743: 4.78553581237793\n","Pérdida época 0: 4.742794036865234\n","Comienzo de epoca 1\n","Prediccion:    [[ 7867  2194  2269 12217  9102  3017 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194  2269  7694 12166 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867 10039  7694  2016     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867 13160  8410  3017 12049  5846  5846  3017  7769 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194  9102  2194 12249 10039 10039 12049 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[ 7867  6458 13888 11436 13927 14455 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867 13366  9102 12853 14084 12166     2     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7804  6635  1682  2016     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [10661  5172 13366  2269   180  7396  5240  3017  4361 12166     2     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867 14422 10039  9102  6204  1687  3017  7323 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 0/743: 4.101018905639648\n","Prediccion:    [[ 7867 10039  3017  2016     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194  3712  2269 12166  7694 12049 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867 10039 10039  2016 10039  2016  2016     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867 10039 12166     2 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  8410  9102  9102  3017  3017  6420 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[ 3651 13390  7866  2016     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7526  2194  3712 12436  9102  3017  9092 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [  745  3271  7717 11059  4624 13615 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 3651 13941  6419     2     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [11229 14572 13307 12811  9102  9037  5771 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 100/743: 3.9987757205963135\n","Prediccion:    [[ 7867  2194 10039  9102  3017 12049 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","      2]\n"," [ 7867  8410  2194 12249 14535  9102  7694 12166 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166     2\n","  12166]\n"," [ 7867  2194  2269 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 7867  8410  2269  9102 10256  3017 12049 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","      2]\n"," [ 7867  8410  2269  2269  9102  9102  3017  3017 12049 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","      2]]\n","Ground truth: [[ 7867 11379  9793  1325  2269 14154 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  6472  2194 12249 14422  9102 12853  6010 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 7867  6390  7261 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  8410  2461  8285 10256  3017  2860 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664 11433 12217 11182  8888  9102  2295  2269  1648 12166     2     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]]\n","Pérdida media en el step 200/743: 3.976984977722168\n","Prediccion:    [[ 7867  2194 10047 12049  2635 10256 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 7867  2194  7694  9102  3017  7769 11059  7867  9102  7694 12166     2\n","  12166 12166 12166 12166 12166]\n"," [ 7867  8410 12166 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 1664  5846  8410  3017  5846  5846  3017 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 7867  2194  2269 12049 10256 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]]\n","Ground truth: [[ 7867 10932  2269 14156  4080  1945 12166     2     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7532  5122  9842  1687  3017  6297 12829  3520  9102  9104 12166     2\n","      0     0     0     0     0]\n"," [ 8573  2009 10559 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 9023  5418 13801  2724 11346 10256  7450 12166     2     0     0     0\n","      0     0     0     0     0]\n"," [10751 13366  2269 10609  3500 12166     2     0     0     0     0     0\n","      0     0     0     0     0]]\n","Pérdida media en el step 300/743: 3.944634437561035\n","Prediccion:    [[ 7867  8410  9102  7694  6010  3017 12166 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 7867 10039 14422 14535  9102  2016  3017  2016  2016     2  2016 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664 12930  8410  3017  9102 13390 11643 10047  9102 10047 12166 12166\n","   2194  2194 12249 12166 12166     2 12166     2     2     2 12166 12166\n","      2]\n"," [ 7867 13160  8410  8410  3017  9102  3017  6420 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 7867  8410  2269  2269 12166     2 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]]\n","Ground truth: [[ 9114 10142  9102 14407 11059  6635  4964 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 9120 10039  8679  5992 10941 13071  1639 10255  2016     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1862  8946  9302   925  7867  2194 11643  3961 13046  1430  8812  7867\n","    936  2194 12249  3362  2269  5654 12166     2     0     0     0     0\n","      0]\n"," [10661  7621 13017  4090  3862  5240  3017   977 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  8410  1861   873 12166     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]]\n","Pérdida media en el step 400/743: 3.915485382080078\n","Prediccion:    [[ 7867  2194  2194 12249 14422  9102  3017 12166 12166 12166 12166     2\n","  12166 12166 12166 12166 12166]\n"," [ 7867  8410  2269 12049  2635 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 7867  8410  2269 10256  3017 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 7867 10039  9102  2269 12049  2016     2  2016  2016 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 7867  2194 12541  7694  2269  3017  9102  3017  7769 11059  3017 12166\n","  12166 12166     2 12166 12166]]\n","Ground truth: [[ 7867  7637  2194 12249   394  7029 10039 13071  6010  3660 12166     2\n","      0     0     0     0     0]\n"," [ 1664 13390  2269 14156  6256 12166     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 5111  8410  8950 11059  1748 12166     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7804  1664 14124  2269  7546  2016     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7532  2194 12541  7694  6755  8046  9102  3017  9075 10256  3376 10039\n","  12853 12166     2     0     0]]\n","Pérdida media en el step 500/743: 3.8889873027801514\n","Prediccion:    [[ 7867 10039 14422  9102  9102  3017  9102  7769  2016  2016  2016     2\n","   2016  2016  2016 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194 10039 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  8410  8410   925 12166 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867 10039  5992  2016  2016     2  2016 12166  2016 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  8410  2269 12049 12166  3017 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[14286 10039 14134   925  9102   219  3017  9655  1325 10039  2016     2\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867  9266  8888 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 1166  8612   124 14084 13390 12166     2     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 2981  7867 10743 10039  2016     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [11229  8492  3017  1932 11059  4571 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 600/743: 3.86674427986145\n","Prediccion:    [[ 7867  2194  2194 12249 14535   925  3017 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194 12249 10039 10039  3017 10485 12166 12049 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194  2269  3520 12049 12166 12166  3017 12166 10039 12166     2\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166     2 12166     2]\n"," [ 7867  8410  9102 10256  3017  3017 10102 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166     2 12166]\n"," [ 7867  2194  3017 10102 10256     2 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[  121  6472  2194 12249 12200 11059  9409 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7526  2194  3712 10047  2330  4029   867  2269 14484 12166     2     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7867  6458  2600  2269  3049 14329 11059   835 10256   925 12166     2\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [  121  3042  2009  6041 13999  3017   705 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7867 13280  8729  6855 12166     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 700/743: 3.8396217823028564\n","Pérdida época 1: 3.82796311378479\n","Comienzo de epoca 2\n","Prediccion:    [[ 7867  2194  9102  7025  6010  3017 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867   936  9102  5992 10039 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867 10039  2016  2016     2  2016 12166  2016 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [10661 13160  8410  2269 12049 11059 11059  3017 10102 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867   936  9102  9102  5992 10039 12166  7769 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[ 7867  6458 13888 11436 13927 14455 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867 13366  9102 12853 14084 12166     2     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7804  6635  1682  2016     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [10661  5172 13366  2269   180  7396  5240  3017  4361 12166     2     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867 14422 10039  9102  6204  1687  3017  7323 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 0/743: 3.736187219619751\n","Prediccion:    [[10661  8410  3017 10256     2  2016  2016  2016  2016 12166  2016 12166\n","   2016  2016 12166 12166 12166 12166 12166 12166 12166]\n"," [10661  2194  3712  3017  9102  7694  6729 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867 10039 11059  9102  3017  2016  2016     2  2016 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166     2]\n"," [ 7867   925  2016     2 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  8410  9102  3017 11059  3017  6420 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[ 3651 13390  7866  2016     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7526  2194  3712 12436  9102  3017  9092 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [  745  3271  7717 11059  4624 13615 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 3651 13941  6419     2     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [11229 14572 13307 12811  9102  9037  5771 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 100/743: 3.6067943572998047\n","Prediccion:    [[ 7867  2194  3017  3017  3017 12049 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  6472  2194 12249 14535  9102  7694 12166 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 7867  2194  2269 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  8410  2269  9102 10256  2269  6729 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 7867 11433  2269  2269  2269  9102  7694  3017 12049 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]]\n","Ground truth: [[ 7867 11379  9793  1325  2269 14154 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  6472  2194 12249 14422  9102 12853  6010 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 7867  6390  7261 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  8410  2461  8285 10256  3017  2860 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664 11433 12217 11182  8888  9102  2295  2269  1648 12166     2     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]]\n","Pérdida media en el step 200/743: 3.585283041000366\n","Prediccion:    [[ 7867  2194  8729  2635  2635 12166 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 7867  2194  7694  9102  3017 10102 12166 10039  9102  3017 12166     2\n","  12166 12166 12166 12166 12166]\n"," [ 7532  1639 12166 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [10661  8410  8410  2269 11059 12166  3017 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [10661  2194  2269 12049 12166 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]]\n","Ground truth: [[ 7867 10932  2269 14156  4080  1945 12166     2     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7532  5122  9842  1687  3017  6297 12829  3520  9102  9104 12166     2\n","      0     0     0     0     0]\n"," [ 8573  2009 10559 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 9023  5418 13801  2724 11346 10256  7450 12166     2     0     0     0\n","      0     0     0     0     0]\n"," [10751 13366  2269 10609  3500 12166     2     0     0     0     0     0\n","      0     0     0     0     0]]\n","Pérdida media en el step 300/743: 3.555114507675171\n","Prediccion:    [[ 7867  8410  9102  7694  3017  3017 12166 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [10661 10039  6458 14134  9102  2016  3017  6420  2016     2  2016  2016\n","   2016  2016  2016 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [11229 13160  8410   925  5846 13390 11643 10047  9102 10047 12166  7867\n","   2194  2194 12249 12166 12166  2635 12166     2     2     2 12166 12166\n","      2]\n"," [10661  8840 12930  8410  2269 10256  3017  4688 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  8410  2269  2635 12166     2 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]]\n","Ground truth: [[ 9114 10142  9102 14407 11059  6635  4964 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 9120 10039  8679  5992 10941 13071  1639 10255  2016     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1862  8946  9302   925  7867  2194 11643  3961 13046  1430  8812  7867\n","    936  2194 12249  3362  2269  5654 12166     2     0     0     0     0\n","      0]\n"," [10661  7621 13017  4090  3862  5240  3017   977 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  8410  1861   873 12166     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]]\n","Pérdida media en el step 400/743: 3.5237739086151123\n","Prediccion:    [[ 7867   936  2194 12249  6458  9102  2269  9102  2269 12166 12166     2\n","  12166 12166 12166 12166 12166]\n"," [ 1664 13390  2269  2635  2635 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [  121  8410  3017 11059  3017 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 1664  1664  2269  3520  2635  2016     2  2016  2016  2016  2016  2016\n","   2016 12166 12166 12166 12166]\n"," [  121  2194  1026  7694  3017  3017  9102  3017  6729 10256  3017 12166\n","  12166 12166     2 12166 12166]]\n","Ground truth: [[ 7867  7637  2194 12249   394  7029 10039 13071  6010  3660 12166     2\n","      0     0     0     0     0]\n"," [ 1664 13390  2269 14156  6256 12166     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 5111  8410  8950 11059  1748 12166     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7804  1664 14124  2269  7546  2016     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7532  2194 12541  7694  6755  8046  9102  3017  9075 10256  3376 10039\n","  12853 12166     2     0     0]]\n","Pérdida media en el step 500/743: 3.495283603668213\n","Prediccion:    [[ 3651 10039 14422  9102  9102  3017  3017  8121  2016  3017  2016     2\n","   2016  2016  2016  2016  2016 12166  2016  2016  2016]\n"," [ 7867  2194 10039 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [  121  8410 11059  2269 12166 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 9120 10039  5992 10039  2016     2  2016  2016  2016  2016  2016  2016\n","   2016  2016  2016  2016 12166 12166 12166 12166 12166]\n"," [ 7867  8410  3017 13361 12166  3017 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[14286 10039 14134   925  9102   219  3017  9655  1325 10039  2016     2\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867  9266  8888 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 1166  8612   124 14084 13390 12166     2     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 2981  7867 10743 10039  2016     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [11229  8492  3017  1932 11059  4571 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 600/743: 3.4702720642089844\n","Prediccion:    [[  121  6472  2194 12249  6458 14084  3017 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166     2 12166     2 12166]\n"," [ 7867  2194  3712 13888  2269  6010 10485 12166 12049 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166     2 12166 12166 12166]\n"," [ 7867  2194  2269  2269  2635  9102 12166  2269 12166  2269 12166     2\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166     2     2 12166 12166 12166     2]\n"," [  121 11433  9102 10256  3017  3017 10102 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166     2 12166     2     2 12166     2     2]\n"," [ 7867  2194  8729 13160 11059     2 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166     2\n","  12166 12166     2     2 12166 12166     2 12166]]\n","Ground truth: [[  121  6472  2194 12249 12200 11059  9409 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7526  2194  3712 10047  2330  4029   867  2269 14484 12166     2     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7867  6458  2600  2269  3049 14329 11059   835 10256   925 12166     2\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [  121  3042  2009  6041 13999  3017   705 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7867 13280  8729  6855 12166     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 700/743: 3.443758249282837\n","Pérdida época 2: 3.433145523071289\n","Comienzo de epoca 3\n","Prediccion:    [[ 7867  7637 13888  7025  9102  3017 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194  9102 12853  6010 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 6327 10039  2635  2016     2  2016 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166     2]\n"," [10661 12930 10256  2269 12049  9102  1325  3017  6729 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867   936  9102  9102  5992  3017  3017  3276 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[ 7867  6458 13888 11436 13927 14455 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867 13366  9102 12853 14084 12166     2     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7804  6635  1682  2016     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [10661  5172 13366  2269   180  7396  5240  3017  4361 12166     2     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867 14422 10039  9102  6204  1687  3017  7323 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 0/743: 3.306662082672119\n","Prediccion:    [[ 6327  2194  3017  2016     2  2016  2016  2016  2016  2016  2016  2016\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 2960  2194  3712  2882  3017  3017 13361 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867 10039  2269  9102  3017 12166 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [10751   925  6419     2  6419  6419 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [  121 13390  9102  3017 11059  3017  6420 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[ 3651 13390  7866  2016     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7526  2194  3712 12436  9102  3017  9092 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [  745  3271  7717 11059  4624 13615 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 3651 13941  6419     2     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [11229 14572 13307 12811  9102  9037  5771 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 100/743: 3.235816478729248\n","Prediccion:    [[ 2960  2194 11059  2269  2269 12049 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  6472  2194 12249 14535  9102 12853 12166 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 7867  2194  2269 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  8410  2269  9102 10256  2269  7396 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664 13390  2269  2269  2269  9102  7694  2269  7769 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","      2]]\n","Ground truth: [[ 7867 11379  9793  1325  2269 14154 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  6472  2194 12249 14422  9102 12853  6010 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 7867  6390  7261 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  8410  2461  8285 10256  3017  2860 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664 11433 12217 11182  8888  9102  2295  2269  1648 12166     2     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]]\n","Pérdida media en el step 200/743: 3.2173802852630615\n","Prediccion:    [[ 7867  2194  2269 14156  2635 12166 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 7867  5122  7694 11059  3017  6729 11059  3017  9102  3017 12166     2\n","  12166 12166 12166 12166 12166]\n"," [ 7532  8410 12166 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [10661 10256  8410   317 11059 10256  3017 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [10751  2194  2269  2635   344 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]]\n","Ground truth: [[ 7867 10932  2269 14156  4080  1945 12166     2     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7532  5122  9842  1687  3017  6297 12829  3520  9102  9104 12166     2\n","      0     0     0     0     0]\n"," [ 8573  2009 10559 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 9023  5418 13801  2724 11346 10256  7450 12166     2     0     0     0\n","      0     0     0     0     0]\n"," [10751 13366  2269 10609  3500 12166     2     0     0     0     0     0\n","      0     0     0     0     0]]\n","Pérdida media en el step 300/743: 3.1897926330566406\n","Prediccion:    [[  121  7637  9102  7694  3017  6635 12166 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 6327 10039  6458  6458  9102  2016  3017  7192  2016     2  2016  2016\n","  12166  2016 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [11229  7192  6472   925  6010  2194 11643 10047  9102  3249 12166 14404\n","  13390  2194 12249  6458 12166  2635 12166     2 12166 12166 12166 12166\n","  12166]\n"," [10661 12741 10256 11433  3017 10256  3017  4688 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  8410  2269  2635 12166     2 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]]\n","Ground truth: [[ 9114 10142  9102 14407 11059  6635  4964 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 9120 10039  8679  5992 10941 13071  1639 10255  2016     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1862  8946  9302   925  7867  2194 11643  3961 13046  1430  8812  7867\n","    936  2194 12249  3362  2269  5654 12166     2     0     0     0     0\n","      0]\n"," [10661  7621 13017  4090  3862  5240  3017   977 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  8410  1861   873 12166     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]]\n","Pérdida media en el step 400/743: 3.164616584777832\n","Prediccion:    [[ 7867   936  2194 12249  7694  9102 10039  9102  3017 12166 12166     2\n","  12166 12166 12166 12166 12166]\n"," [ 1664 13390  2269  2635  2635 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [10661  8410  2269 11059  3017 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 6327  1664 10559  1664  2635  2016     2  2016  2016  2016  2016  2016\n","   2016  2016  2016 12166  2016]\n"," [  121  2194  1026  7694 14388  3017  9102  3017  6729 10256  3017 12166\n","  12166 12166     2 12166 12166]]\n","Ground truth: [[ 7867  7637  2194 12249   394  7029 10039 13071  6010  3660 12166     2\n","      0     0     0     0     0]\n"," [ 1664 13390  2269 14156  6256 12166     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 5111  8410  8950 11059  1748 12166     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7804  1664 14124  2269  7546  2016     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7532  2194 12541  7694  6755  8046  9102  3017  9075 10256  3376 10039\n","  12853 12166     2     0     0]]\n","Pérdida media en el step 500/743: 3.141652822494507\n","Prediccion:    [[ 3651 10039 14134   925  9102  3017  1639  6420  2016  6635  2016     2\n","   2016  2016  2016  2016  2016  2016  2016 12166  2016]\n"," [ 7867  2194 10039 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [10751  8410  2269 11383 12166 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 9120 10039  5992  2016  2016     2  2016  2016  2016  2016  2016  2016\n","   2016 12166  2016  2016 12166 12166 12166 12166 12166]\n"," [ 2960  2194  3017 13361 12166  3017 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[14286 10039 14134   925  9102   219  3017  9655  1325 10039  2016     2\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867  9266  8888 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 1166  8612   124 14084 13390 12166     2     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 2981  7867 10743 10039  2016     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [11229  8492  3017  1932 11059  4571 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 600/743: 3.1246557235717773\n","Prediccion:    [[  121  6472  2194 12249 14422 10039 12166 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166     2     2]\n"," [ 7867  2194 12249 13888 10039  3017 10485 12166 12049 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166     2 12166     2     2]\n"," [ 7867  2194  2269 12217  2635 12166 12166  2269 12166  2269 12166     2\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166     2     2     2     2     2]\n"," [  121 11433  9102 11059  3017  3017 10102 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166     2 12166 12166]\n"," [ 7867  2194  3017  8128 11059     2 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166     2     2]]\n","Ground truth: [[  121  6472  2194 12249 12200 11059  9409 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7526  2194  3712 10047  2330  4029   867  2269 14484 12166     2     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7867  6458  2600  2269  3049 14329 11059   835 10256   925 12166     2\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [  121  3042  2009  6041 13999  3017   705 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7867 13280  8729  6855 12166     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 700/743: 3.10530948638916\n","Pérdida época 3: 3.097316265106201\n","Comienzo de epoca 4\n","Prediccion:    [[ 7867  7637 13888  7025  9102  3017 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  8835  9102 12853 10039 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 6327 14084 11059  2016     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [10661  8096 13390  2269  2635  7396  1325  2269  6729 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867 14422  9102  9102  5992  3017  3017  7935 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[ 7867  6458 13888 11436 13927 14455 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867 13366  9102 12853 14084 12166     2     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7804  6635  1682  2016     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [10661  5172 13366  2269   180  7396  5240  3017  4361 12166     2     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867 14422 10039  9102  6204  1687  3017  7323 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 0/743: 3.0120418071746826\n","Prediccion:    [[ 6327  9398 10039 11059     2  2016  2016  2016  2016  2016  2016  2016\n","   2016  2016  2016  2016  2016  2016 12166 12166 12166]\n"," [ 7867  2194  3712  5992  3017  3017 13377 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7532 10039  9102 12166  3017 12166 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [10751   925  6419     2  6419  6419 12166 12166 12166  6419  6419 12166\n","   6419 12166  6419 12166 12166 12166 12166  6419 12166]\n"," [11229 13068  3017 14145 11059  3017  8128 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[ 3651 13390  7866  2016     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7526  2194  3712 12436  9102  3017  9092 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [  745  3271  7717 11059  4624 13615 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 3651 13941  6419     2     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [11229 14572 13307 12811  9102  9037  5771 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 100/743: 2.9501662254333496\n","Prediccion:    [[ 2960  2194 13927  2269  2269  8163 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  6472  2194 12249 14422  9102 12853  6010 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 7867  2194  8584 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  8410  3017  3017 10256  3017  7396 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664 11433 12217  2269  2269  9102  2269  2269  1358 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]]\n","Ground truth: [[ 7867 11379  9793  1325  2269 14154 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  6472  2194 12249 14422  9102 12853  6010 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 7867  6390  7261 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  8410  2461  8285 10256  3017  2860 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664 11433 12217 11182  8888  9102  2295  2269  1648 12166     2     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]]\n","Pérdida media en el step 200/743: 2.935756206512451\n","Prediccion:    [[ 7867  2194  2269 14156  3961 12166 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 2960  6458  7694  9102  3017  4688 11059 10256  9102  3017 12166     2\n","  12166 12166 12166 12166 12166]\n"," [ 7532  6635 10559 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [10661  8410 13801  9102  1325 10256  3017 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [10751  2194  2269 14156 13046 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]]\n","Ground truth: [[ 7867 10932  2269 14156  4080  1945 12166     2     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7532  5122  9842  1687  3017  6297 12829  3520  9102  9104 12166     2\n","      0     0     0     0     0]\n"," [ 8573  2009 10559 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 9023  5418 13801  2724 11346 10256  7450 12166     2     0     0     0\n","      0     0     0     0     0]\n"," [10751 13366  2269 10609  3500 12166     2     0     0     0     0     0\n","      0     0     0     0     0]]\n","Pérdida media en el step 300/743: 2.91316294670105\n","Prediccion:    [[13202 11433  9102  7694  6635  6635 12166 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166     2\n","  12166]\n"," [12008 10039  6458  5992  9102  9102  3017 12705  2016     2 12166  2016\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1862  7673  6010   925  9102  7637 11643 10047  9102 10477 12166 14404\n","   2194  2194 12249  7717 12166 12049 12166     2 12166 12166 12166 12166\n","  12166]\n"," [10661  8096 10256  8410  3017 11059  3017  4688 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  8410  2269  3049 12166     2 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166     2\n","      2]]\n","Ground truth: [[ 9114 10142  9102 14407 11059  6635  4964 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 9120 10039  8679  5992 10941 13071  1639 10255  2016     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1862  8946  9302   925  7867  2194 11643  3961 13046  1430  8812  7867\n","    936  2194 12249  3362  2269  5654 12166     2     0     0     0     0\n","      0]\n"," [10661  7621 13017  4090  3862  5240  3017   977 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  8410  1861   873 12166     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]]\n","Pérdida media en el step 400/743: 2.892573356628418\n","Prediccion:    [[ 7867  2194  2194 12249  7694  9102 10039 12166  6010 12166 12166     2\n","  12166 12166 12166 12166 12166]\n"," [ 1664 13390  2269  2635  2635 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [10661  8410 11059 11059  3017 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 3651  1664 14124  1664  3249  2016     2  2016  2016  2016  2016  2016\n","   2016  2016  2016  2016  2016]\n"," [  121  2194  1026  7694 11059  3017  9102  3017  6729 10256  7396  5846\n","  14309 12166     2 12166 12166]]\n","Ground truth: [[ 7867  7637  2194 12249   394  7029 10039 13071  6010  3660 12166     2\n","      0     0     0     0     0]\n"," [ 1664 13390  2269 14156  6256 12166     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 5111  8410  8950 11059  1748 12166     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7804  1664 14124  2269  7546  2016     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7532  2194 12541  7694  6755  8046  9102  3017  9075 10256  3376 10039\n","  12853 12166     2     0     0]]\n","Pérdida media en el step 500/743: 2.8752787113189697\n","Prediccion:    [[ 3651 10039 14422   925  9102  2882  3017  8121  2016   925  2016     2\n","   2016  2016  2016  2016 12166 12166 12166 12166 12166]\n"," [ 7867  2194 10039 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [13202  2194 12166  2269 12166 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 9120 10039  2882 10039  2016     2  2016  2016  2016  2016  2016  2016\n","   2016  2016 12166 12166 12166 12166 12166 12166 12166]\n"," [ 2960 13068  3017  7769  9102  2269 13608     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[14286 10039 14134   925  9102   219  3017  9655  1325 10039  2016     2\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867  9266  8888 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 1166  8612   124 14084 13390 12166     2     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 2981  7867 10743 10039  2016     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [11229  8492  3017  1932 11059  4571 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 600/743: 2.8619091510772705\n","Prediccion:    [[  121  6472  2194 12249 14535  9037  9037 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166     2     2     2     2 12166 12166]\n"," [ 8405  2194  3712 10047  7694  3017  2010 12166  7769 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166     2 12166     2]\n"," [ 7867  2194  2269 12217  2635 12166 12166  7866 10256 12166 12166     2\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166     2 12166 12166 12166 12166     2     2]\n"," [  121 13068  9102 11059  3017  3017 13819 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166     2 12166 12166     2]\n"," [ 7867  2194   530  8128 12166     2 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166     2     2     2     2     2]]\n","Ground truth: [[  121  6472  2194 12249 12200 11059  9409 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7526  2194  3712 10047  2330  4029   867  2269 14484 12166     2     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7867  6458  2600  2269  3049 14329 11059   835 10256   925 12166     2\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [  121  3042  2009  6041 13999  3017   705 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7867 13280  8729  6855 12166     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 700/743: 2.84512996673584\n","Pérdida época 4: 2.8382620811462402\n","Comienzo de epoca 5\n","Prediccion:    [[ 7867  7637 13888  7025 11059  3017 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  8835  9102 12853  2352 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 6327 14084  2016  2016     2  2016  2016  2016 12166 12166  2016  2016\n","  12166 12166 12166 12166  2016 12166 12166  2016     2]\n"," [10661  8096 13390  2269 12049  7396  1325  3017  6729 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867 14422 10039  9102  5992  3017  3017  4979 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[ 7867  6458 13888 11436 13927 14455 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867 13366  9102 12853 14084 12166     2     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7804  6635  1682  2016     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [10661  5172 13366  2269   180  7396  5240  3017  4361 12166     2     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867 14422 10039  9102  6204  1687  3017  7323 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 0/743: 2.7908878326416016\n","Prediccion:    [[ 6327  4659 10039 11059     2  2016  2016  2016  2016  2016  2016  2016\n","   2016 12166  2016  2016  2016  2016 12166 12166 12166]\n"," [ 2960  2194  3712  5992 13071  3017 13361 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 2960 10039  9102  9102  3017 12166 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [10751  2269  6419     2  6419 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [11229 14327 11059 13927 11059  3017  6371 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[ 3651 13390  7866  2016     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7526  2194  3712 12436  9102  3017  9092 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [  745  3271  7717 11059  4624 13615 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 3651 13941  6419     2     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [11229 14572 13307 12811  9102  9037  5771 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 100/743: 2.708055019378662\n","Prediccion:    [[ 2960  2194  9102  2269  2269  8163 12166     2  7396 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  6472  2194 12249 14422  9102 14535  6010 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 7867  2194  2269 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  8410   342  9102 10256  3017  7935 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664 11433 12217  2269  2269  1325  2269  2269  1358 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]]\n","Ground truth: [[ 7867 11379  9793  1325  2269 14154 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  6472  2194 12249 14422  9102 12853  6010 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 7867  6390  7261 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  8410  2461  8285 10256  3017  2860 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664 11433 12217 11182  8888  9102  2295  2269  1648 12166     2     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]]\n","Pérdida media en el step 200/743: 2.696765422821045\n","Prediccion:    [[ 7867  6458  2269 14156 11722  9102 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 2960  5122  6458  9102  3017  4571  9102 10039  9102  3017 12166     2\n","  12166 12166 12166 12166 12166]\n"," [ 8573 10559  8729 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [10661  5418 13801  2173  9102 10256  3017 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [10751 13366  2269  2635  8379 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]]\n","Ground truth: [[ 7867 10932  2269 14156  4080  1945 12166     2     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7532  5122  9842  1687  3017  6297 12829  3520  9102  9104 12166     2\n","      0     0     0     0     0]\n"," [ 8573  2009 10559 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 9023  5418 13801  2724 11346 10256  7450 12166     2     0     0     0\n","      0     0     0     0     0]\n"," [10751 13366  2269 10609  3500 12166     2     0     0     0     0     0\n","      0     0     0     0     0]]\n","Pérdida media en el step 300/743: 2.678118944168091\n","Prediccion:    [[13988 11657  9102  7694  6635  6635 12166 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [12008 10039  6458  6458  9102  2016  3017  9675  2016     2  2016  2016\n","  12166  2016  2016 12166  2016  2016  2016 12166 12166 12166 12166  2016\n","  12166]\n"," [ 1862  7192  6475   925  6010  7637 11643 14134  5846 10047 12166  7867\n","  13390  2194 12249  6458 12166 12049 12166     2 12166 12166 12166 12166\n","  12166]\n"," [10661  4571 10256  9425  3017 13927  3017  5767 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  8410 11059  1869 12166     2 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]]\n","Ground truth: [[ 9114 10142  9102 14407 11059  6635  4964 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 9120 10039  8679  5992 10941 13071  1639 10255  2016     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1862  8946  9302   925  7867  2194 11643  3961 13046  1430  8812  7867\n","    936  2194 12249  3362  2269  5654 12166     2     0     0     0     0\n","      0]\n"," [10661  7621 13017  4090  3862  5240  3017   977 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  8410  1861   873 12166     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]]\n","Pérdida media en el step 400/743: 2.6590652465820312\n","Prediccion:    [[ 7867  7637  2194 12249 12200  9398  6010 12166  6010 12166 12166     2\n","  12166 12166 12166 12166 12166]\n"," [ 1664 13390  2269  2635 13608 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [10661  8410 11059 11059 12777 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 7804 14084 14124 14124  2635  2016     2  2016  2016  2016  2016  2016\n","   2016  2016  2016  2016  2016]\n"," [  121  2194  1026  7694  8743  9102  9102  3017  6729 10256  7396  6010\n","   7637 12166     2 12166 12166]]\n","Ground truth: [[ 7867  7637  2194 12249   394  7029 10039 13071  6010  3660 12166     2\n","      0     0     0     0     0]\n"," [ 1664 13390  2269 14156  6256 12166     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 5111  8410  8950 11059  1748 12166     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7804  1664 14124  2269  7546  2016     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7532  2194 12541  7694  6755  8046  9102  3017  9075 10256  3376 10039\n","  12853 12166     2     0     0]]\n","Pérdida media en el step 500/743: 2.6414833068847656\n","Prediccion:    [[ 9120 10039 14134  9102  9102 14327  3017 11765  2016   925  2016     2\n","   2016  2016  2016 12166  2016  2016  2016  2016  2016]\n"," [ 7867  2194 10039 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [10751  8410  2269  2269 12166  2269     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 2981  7867  3779 10039  2016     2  2016  2016  2016  2016  2016  2016\n","   2016  2016 12166  2016  2016  2016 12166 12166  2016]\n"," [ 2960   521  3017 14484 11059  7866 12048     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[14286 10039 14134   925  9102   219  3017  9655  1325 10039  2016     2\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867  9266  8888 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 1166  8612   124 14084 13390 12166     2     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 2981  7867 10743 10039  2016     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [11229  8492  3017  1932 11059  4571 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 600/743: 2.627291679382324\n","Prediccion:    [[  121  6472  2194 12249 14134 11059 14084 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 8405  2194  3712 12628  2259  3017  2010 10485  1358 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194  2269 12217  2635 12166 12166  2269 12166  6488 12166     2\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166     2]\n"," [  121   521  9102 10256  3017  3017 10430 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194   530  3463 11059     2 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[  121  6472  2194 12249 12200 11059  9409 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7526  2194  3712 10047  2330  4029   867  2269 14484 12166     2     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7867  6458  2600  2269  3049 14329 11059   835 10256   925 12166     2\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [  121  3042  2009  6041 13999  3017   705 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7867 13280  8729  6855 12166     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 700/743: 2.61118483543396\n","Pérdida época 5: 2.604687452316284\n","Comienzo de epoca 6\n","Prediccion:    [[ 7867  2194 13888 11436 11059  9037 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194  9102 12853 14084 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7804  6010  2016  2016     2  2016  2016  2016  2016  2016  2016  2016\n","   2016  2016  2016  2016  2016  2016  2016  2016  2016]\n"," [10661 11765  9425  2269 12049  7396  1325  3017  2010 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867 14422 10039  9102  2882  3017  3017  1208 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[ 7867  6458 13888 11436 13927 14455 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867 13366  9102 12853 14084 12166     2     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7804  6635  1682  2016     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [10661  5172 13366  2269   180  7396  5240  3017  4361 12166     2     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867 14422 10039  9102  6204  1687  3017  7323 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 0/743: 2.509995698928833\n","Prediccion:    [[ 6327 13745  1397  2016     2  2016  2016  2016  2016  2016  2016  2016\n","   2016  2016  2016  2016  2016  2016  2016  2016  2016]\n"," [ 2960  2194  3712  5992 13071  3017 13377 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 2960 14084  9102  9102  3017 12166 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [10751  2269  6419     2  6419  6419  6419  6419 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [11229 14327 11059 11059 11059  9037 11057 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[ 3651 13390  7866  2016     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7526  2194  3712 12436  9102  3017  9092 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [  745  3271  7717 11059  4624 13615 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 3651 13941  6419     2     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [11229 14572 13307 12811  9102  9037  5771 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 100/743: 2.47660231590271\n","Prediccion:    [[ 2960  2194  7029  2269  2269  8163 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  6472  2194 12249 14422  9102 12853  6010 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 7867  2194  1562 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  8410   342  3017 10256  3017  7935 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664 11433 12217  5808  2269  1325  2269  2269  1358 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]]\n","Ground truth: [[ 7867 11379  9793  1325  2269 14154 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  6472  2194 12249 14422  9102 12853  6010 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 7867  6390  7261 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  8410  2461  8285 10256  3017  2860 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664 11433 12217 11182  8888  9102  2295  2269  1648 12166     2     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]]\n","Pérdida media en el step 200/743: 2.4678409099578857\n","Prediccion:    [[ 7867 10932  2269  2635  3049  1325 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 2960 14341  2882  9102  3017  6729  9102 10256  9102  7694 12166     2\n","  12166 12166 12166 12166 12166]\n"," [ 8573 10559  8729 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [10661  5418  8410   317 12166 12166  3017 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [10751 13366  2269 12049  5450 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]]\n","Ground truth: [[ 7867 10932  2269 14156  4080  1945 12166     2     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7532  5122  9842  1687  3017  6297 12829  3520  9102  9104 12166     2\n","      0     0     0     0     0]\n"," [ 8573  2009 10559 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 9023  5418 13801  2724 11346 10256  7450 12166     2     0     0     0\n","      0     0     0     0     0]\n"," [10751 13366  2269 10609  3500 12166     2     0     0     0     0     0\n","      0     0     0     0     0]]\n","Pérdida media en el step 300/743: 2.449962854385376\n","Prediccion:    [[  277 11012  9102  7694 11059  6635   313 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [12008 10039  6458  6458  9102  2016  3017 12705  2016     2  2016  2016\n","   2016 12166  2016  2016  2016  2016  2016  2016 12166  2016 12166  2016\n","   2016]\n"," [ 1862  7192 14018   925  9102  2194 11643 10047   664  2369 12166  7867\n","    936  2194 12249 12166 12166 12166 12166     2 12166 12166 12166 12166\n","  12166]\n"," [10661 12157 12930  8410  3017  9102  3017  3645 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  8410  8768  7475 12166     2 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]]\n","Ground truth: [[ 9114 10142  9102 14407 11059  6635  4964 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 9120 10039  8679  5992 10941 13071  1639 10255  2016     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1862  8946  9302   925  7867  2194 11643  3961 13046  1430  8812  7867\n","    936  2194 12249  3362  2269  5654 12166     2     0     0     0     0\n","      0]\n"," [10661  7621 13017  4090  3862  5240  3017   977 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  8410  1861   873 12166     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]]\n","Pérdida media en el step 400/743: 2.4316565990448\n","Prediccion:    [[ 7867  7637  2194 12249  6754 13927  6010 12166  6010 12166 12166     2\n","  12166 12166 12166 12166 12166]\n"," [ 1664 13390  2269  9070  9070 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [10661  8410  2269 11059 12777 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 3651 14084 14124 14124  4934  2016     2  2016  2016  2016  2016  2016\n","   2016  2016  2016  2016  2016]\n"," [  121  2194 12541  7694   342  9102  9102 12853  6729 10256  7396   124\n","  12166 12166     2 12166 12166]]\n","Ground truth: [[ 7867  7637  2194 12249   394  7029 10039 13071  6010  3660 12166     2\n","      0     0     0     0     0]\n"," [ 1664 13390  2269 14156  6256 12166     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 5111  8410  8950 11059  1748 12166     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7804  1664 14124  2269  7546  2016     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7532  2194 12541  7694  6755  8046  9102  3017  9075 10256  3376 10039\n","  12853 12166     2     0     0]]\n","Pérdida media en el step 500/743: 2.416574239730835\n","Prediccion:    [[ 3651 10039 14134  9102  9102  5162  3017  2671  2016 10039  2016     2\n","   2016  2016  2016  2016  2016  2016  2016  2016  2016]\n"," [ 7867  2194 10039 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [12329  2194  2269  2269 12166  2269     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 2981 10039  2882 10039  2016     2  2016  2016  2016  2016  2016  2016\n","   2016  2016  2016  2016  2016  2016  2016  2016  2016]\n"," [  121   521  3017  7769 11059  7866 12048     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[14286 10039 14134   925  9102   219  3017  9655  1325 10039  2016     2\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867  9266  8888 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 1166  8612   124 14084 13390 12166     2     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 2981  7867 10743 10039  2016     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [11229  8492  3017  1932 11059  4571 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 600/743: 2.4030449390411377\n","Prediccion:    [[  121  6472  2194 12249 14422 11059  7800 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 8405  2194  3712 12628  7411  2269 10485 10485  1967 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194  2269 13505  2635 12166 12166  8729 12166  6488 12166     2\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166     2 12166]\n"," [  121 13068  9102 10256  3017  3017 10430 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194 10039 11057 11059     2 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166     2 12166]]\n","Ground truth: [[  121  6472  2194 12249 12200 11059  9409 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7526  2194  3712 10047  2330  4029   867  2269 14484 12166     2     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7867  6458  2600  2269  3049 14329 11059   835 10256   925 12166     2\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [  121  3042  2009  6041 13999  3017   705 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7867 13280  8729  6855 12166     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 700/743: 2.388982057571411\n","Pérdida época 6: 2.382821798324585\n","Comienzo de epoca 7\n","Prediccion:    [[ 7867  6458 13888 11436 11059 14455 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867 11472  9102 14535 14084 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7804  6635  6635  2016     2  2016  2016  2016  2016  2016  2016  2016\n","   2016  2016  2016  2016  2016  2016  2016  2016  2016]\n"," [10661  5172 13366  2269 12049  7396 13071  3017 12215 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867 14422 10039  9102  8734  3017  3017  1208 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[ 7867  6458 13888 11436 13927 14455 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867 13366  9102 12853 14084 12166     2     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7804  6635  1682  2016     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [10661  5172 13366  2269   180  7396  5240  3017  4361 12166     2     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867 14422 10039  9102  6204  1687  3017  7323 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 0/743: 2.3136909008026123\n","Prediccion:    [[ 6327  9398 10039  2016     2  2016  2016  2016  2016  2016  2016  2016\n","   2016  2016  2016  2016  2016  2016  2016  2016  2016]\n"," [ 2960  2194  3712  5992 13071  3017 12574 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 2960 14084  7029  7029  6010 13615 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7532  2269  6419     2  6419  6419  6419  6419  6419  6419 12166 12166\n","   6419 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [11229 14327 11059 11057 11059  9037 11057 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[ 3651 13390  7866  2016     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7526  2194  3712 12436  9102  3017  9092 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [  745  3271  7717 11059  4624 13615 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 3651 13941  6419     2     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [11229 14572 13307 12811  9102  9037  5771 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 100/743: 2.27272629737854\n","Prediccion:    [[ 2960  2194  1325  2269  2269 14154 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  6472  2194 12249 14422  9102 12853  6010 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 7867  2194 11060 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  8410  8729  3017 10256  3017  7935 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664 11433 12217  5808 12393  1325  2295  2269  8592 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]]\n","Ground truth: [[ 7867 11379  9793  1325  2269 14154 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  6472  2194 12249 14422  9102 12853  6010 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 7867  6390  7261 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  8410  2461  8285 10256  3017  2860 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664 11433 12217 11182  8888  9102  2295  2269  1648 12166     2     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]]\n","Pérdida media en el step 200/743: 2.2669219970703125\n","Prediccion:    [[ 7867  2194  2269 14156   668  1945 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 7532  5122  5162  9102  3017  4688 12829 11120  9102  9104 12166     2\n","  12166 12166 12166 12166 12166]\n"," [ 8573 10559 10559 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [10661  5418  8410   317  9102 12166  3017 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [10751 13366  2269 12049  8440 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]]\n","Ground truth: [[ 7867 10932  2269 14156  4080  1945 12166     2     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7532  5122  9842  1687  3017  6297 12829  3520  9102  9104 12166     2\n","      0     0     0     0     0]\n"," [ 8573  2009 10559 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 9023  5418 13801  2724 11346 10256  7450 12166     2     0     0     0\n","      0     0     0     0     0]\n"," [10751 13366  2269 10609  3500 12166     2     0     0     0     0     0\n","      0     0     0     0     0]]\n","Pérdida media en el step 300/743: 2.2498373985290527\n","Prediccion:    [[  277 10142  9102  7694 11059  6635   313 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [12008 10039  6458  6458  9102  2016  3017  9675  2016     2  2016  2016\n","   2016  2016  2016  2016  2016  2016  2016  2016  2016  2016  2016  2016\n","   2016]\n"," [ 1862  4934 14100   925  9102  2194 11643 10047  5846  2369 12166  7867\n","   2194  2194 12249  6458 12166 10097 12166     2 12166 12166 12166 12166\n","  12166]\n"," [10661 12157 10006 13390  8888 13071  3017  5767 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  8410 11060  7475 12166     2 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]]\n","Ground truth: [[ 9114 10142  9102 14407 11059  6635  4964 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 9120 10039  8679  5992 10941 13071  1639 10255  2016     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1862  8946  9302   925  7867  2194 11643  3961 13046  1430  8812  7867\n","    936  2194 12249  3362  2269  5654 12166     2     0     0     0     0\n","      0]\n"," [10661  7621 13017  4090  3862  5240  3017   977 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  8410  1861   873 12166     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]]\n","Pérdida media en el step 400/743: 2.2328743934631348\n","Prediccion:    [[ 7867  7637  2194 12249  4180  7029  6010  7029  6010 12166 12166     2\n","  12166 12166 12166 12166 12166]\n"," [ 1664 13390  2269   668  1614 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 7377  8410 11059 11059  7800 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 7804  1664 14124 14124  9275  2016     2  2016  2016  2016  2016  2016\n","   2016  2016  2016  2016  2016]\n"," [  121  2194 12541  7694  8743  3017  9102  3017  6729 10256  5418 12166\n","  12166 12166     2 12166 12166]]\n","Ground truth: [[ 7867  7637  2194 12249   394  7029 10039 13071  6010  3660 12166     2\n","      0     0     0     0     0]\n"," [ 1664 13390  2269 14156  6256 12166     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 5111  8410  8950 11059  1748 12166     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7804  1664 14124  2269  7546  2016     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7532  2194 12541  7694  6755  8046  9102  3017  9075 10256  3376 10039\n","  12853 12166     2     0     0]]\n","Pérdida media en el step 500/743: 2.2189383506774902\n","Prediccion:    [[ 9120 10039 14134   925  9102 14327  3017  2671  9102   925  2016     2\n","   2016  2016  2016  2016  2016  2016  2016  2016  2016]\n"," [ 7867  2194 10039 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [12329 13390  2269  2269 12166 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 2981  7867  9239 10039  2016     2  2016  2016  2016  2016  2016  2016\n","   2016  2016  2016  2016  2016  2016  2016  2016  2016]\n"," [  121   521  3017  1932 11059   724 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[14286 10039 14134   925  9102   219  3017  9655  1325 10039  2016     2\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867  9266  8888 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 1166  8612   124 14084 13390 12166     2     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 2981  7867 10743 10039  2016     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [11229  8492  3017  1932 11059  4571 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 600/743: 2.20701003074646\n","Prediccion:    [[  121  6472  2194 12249 12200 11059  7800 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 8405  2194  3712 12628  4029  3017 10485 10485 14484 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194  2269 13505  2635  8379 12166  8729 12166  6488 12166     2\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [  121 13068  9102 10256  3017  3017 13017 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194   530 11057 12166     2 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[  121  6472  2194 12249 12200 11059  9409 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7526  2194  3712 10047  2330  4029   867  2269 14484 12166     2     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7867  6458  2600  2269  3049 14329 11059   835 10256   925 12166     2\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [  121  3042  2009  6041 13999  3017   705 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7867 13280  8729  6855 12166     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 700/743: 2.1930935382843018\n","Pérdida época 7: 2.1876254081726074\n","Comienzo de epoca 8\n","Prediccion:    [[ 7867  6458 13888 10477 11059 14455 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867 13366  9102 12853 14084 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7804  6635 13878  2016     2  2016  2016  2016  2016  2016  2016  2016\n","   2016  2016  2016  2016  2016  2016  2016  2016  2016]\n"," [10661   754 10256 12217  8163  7396  9102  3017 12036 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867 14422 10039  9102  5162  3017  3017  6297 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[ 7867  6458 13888 11436 13927 14455 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867 13366  9102 12853 14084 12166     2     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7804  6635  1682  2016     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [10661  5172 13366  2269   180  7396  5240  3017  4361 12166     2     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867 14422 10039  9102  6204  1687  3017  7323 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 0/743: 2.1363608837127686\n","Prediccion:    [[ 6327  9398 10039  2016     2  2016  2016  2016  2016  2016  2016  2016\n","   2016  2016  2016  2016  2016  2016  2016  2016  2016]\n"," [  121  2194  3712  5992 13071  3017  1533 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 2960 14084  9102 13615 13615 13615 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [10751  2269  6419     2  6419  6419  6419  6419  6419  6419  6419  6419\n","   6419 12166  6419 12166 12166 12166 12166 12166 12166]\n"," [11229 14327 13307  7192 11059 13307 11057 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[ 3651 13390  7866  2016     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7526  2194  3712 12436  9102  3017  9092 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [  745  3271  7717 11059  4624 13615 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 3651 13941  6419     2     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [11229 14572 13307 12811  9102  9037  5771 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 100/743: 2.08915638923645\n","Prediccion:    [[ 7526  2194  1325  2352  2269 14154 12166     2 12166 12166 14084 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166     2 12166     2 12166\n","      2]\n"," [ 1664  6472  2194 12249 14422  9102 12853  6010 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 7867  2194  1562 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  8410  8285  3017 10256  3017  2860 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664 11433 12217  5808 12393  1325  2295  2269 13895 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]]\n","Ground truth: [[ 7867 11379  9793  1325  2269 14154 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  6472  2194 12249 14422  9102 12853  6010 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 7867  6390  7261 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  8410  2461  8285 10256  3017  2860 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664 11433 12217 11182  8888  9102  2295  2269  1648 12166     2     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]]\n","Pérdida media en el step 200/743: 2.0866048336029053\n","Prediccion:    [[ 7867 10932  2269 14156  6256  1945 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 7532  5122  5162  9102  3017  4688 12829 10039  9102  4967 12166     2\n","  12166 12166 12166 12166 12166]\n"," [ 8573 10559 10559 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [10661  5418  8410  3017 12393 10256  3017 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [10751 13366  2269 14156  8379 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]]\n","Ground truth: [[ 7867 10932  2269 14156  4080  1945 12166     2     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7532  5122  9842  1687  3017  6297 12829  3520  9102  9104 12166     2\n","      0     0     0     0     0]\n"," [ 8573  2009 10559 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 9023  5418 13801  2724 11346 10256  7450 12166     2     0     0     0\n","      0     0     0     0     0]\n"," [10751 13366  2269 10609  3500 12166     2     0     0     0     0     0\n","      0     0     0     0     0]]\n","Pérdida media en el step 300/743: 2.0694146156311035\n","Prediccion:    [[  277  7637  9102  7694 11059  6635   313 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [12008 10039  6458  9595  9102 13071  3017  9675  2016     2  2016  2016\n","  12166  2016  2016  2016  2016  2016  2016 12166  2016  2016  2016  2016\n","   2016]\n"," [ 1862  4934  8850   925  9102  2194 11643 10047   664  2369 12166 14404\n","   2194  2194 12249 12166 12166  6859 12166     2 12166 12166 12166 12166\n","  12166]\n"," [10661 12157 13639   521  3017  5240  3017   977 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  8410  1861  5865 12166     2 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]]\n","Ground truth: [[ 9114 10142  9102 14407 11059  6635  4964 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 9120 10039  8679  5992 10941 13071  1639 10255  2016     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1862  8946  9302   925  7867  2194 11643  3961 13046  1430  8812  7867\n","    936  2194 12249  3362  2269  5654 12166     2     0     0     0     0\n","      0]\n"," [10661  7621 13017  4090  3862  5240  3017   977 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  8410  1861   873 12166     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]]\n","Pérdida media en el step 400/743: 2.0521111488342285\n","Prediccion:    [[ 7867  7637  2194 12249  3268  9102  6010 11059  6010 12166 12166     2\n","  12166 12166 12166 12166 12166]\n"," [ 1664 13390  2269 14156  7475 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [10661  8410  9502 11059 12777 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 3651  1664 14124  7546 14621  2016     2  2016  2016  2016  2016  2016\n","   2016  2016  2016  2016  2016]\n"," [  121  2194 12541  7694  8743 13927  9102  2882  6729 10256  5418 12166\n","  12853 12166     2 12166 12166]]\n","Ground truth: [[ 7867  7637  2194 12249   394  7029 10039 13071  6010  3660 12166     2\n","      0     0     0     0     0]\n"," [ 1664 13390  2269 14156  6256 12166     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 5111  8410  8950 11059  1748 12166     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7804  1664 14124  2269  7546  2016     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7532  2194 12541  7694  6755  8046  9102  3017  9075 10256  3376 10039\n","  12853 12166     2     0     0]]\n","Pérdida media en el step 500/743: 2.03912353515625\n","Prediccion:    [[ 9120 10039 14134   925  9102   596  3017  6298  1325  1963  2016     2\n","   2016  2016  2016  2016  2016  2016  2016  2016  2016]\n"," [ 7867  2194  8888 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 9345 13390  9051  2269 13390 14728     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 3910  7867  4202 10039  2016     2  2016  2016  2016  2016  2016  2016\n","   2016  2016  2016  2016  2016  2016  2016  2016  2016]\n"," [ 7867   521  3017  1932 11059   724 12048     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[14286 10039 14134   925  9102   219  3017  9655  1325 10039  2016     2\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867  9266  8888 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 1166  8612   124 14084 13390 12166     2     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 2981  7867 10743 10039  2016     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [11229  8492  3017  1932 11059  4571 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 600/743: 2.0282022953033447\n","Prediccion:    [[  121  6472  2194 12249 12200 11059  9409 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 2960  2194  3712 10047  7411  4029 10485 10485 12049 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194  2269 13505 14156  9102 12166  8729 12166 10039 12166     2\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [  121 13068  2009 10256  3017  3017 13148 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194 10039  8630 12166     2 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[  121  6472  2194 12249 12200 11059  9409 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7526  2194  3712 10047  2330  4029   867  2269 14484 12166     2     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7867  6458  2600  2269  3049 14329 11059   835 10256   925 12166     2\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [  121  3042  2009  6041 13999  3017   705 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7867 13280  8729  6855 12166     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 700/743: 2.015212297439575\n","Pérdida época 8: 2.0103566646575928\n","Comienzo de epoca 9\n","Prediccion:    [[ 7867  6458 13888  7967 11059 11891 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867 13366  9102 12853  6010 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7804  6635  2016  2016     2  2016  2016  2016  2016  2016  2016  2016\n","   2016  2016  2016  2016  2016  2016  2016  2016  2016]\n"," [10661  5172 13390 12217 14156  7396  5240  3017  4361 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867 14422  9102  9102  5162  3017  3017  6297 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[ 7867  6458 13888 11436 13927 14455 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867 13366  9102 12853 14084 12166     2     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7804  6635  1682  2016     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [10661  5172 13366  2269   180  7396  5240  3017  4361 12166     2     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867 14422 10039  9102  6204  1687  3017  7323 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 0/743: 1.9605344533920288\n","Prediccion:    [[ 6327  9398 10039  2016     2  2016  2016  2016  2016  2016  2016  2016\n","   2016  2016  2016  2016  2016  2016  2016  2016  2016]\n"," [ 2960  2194  3712  5992  3017  3017  5430 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 2960 10039 11059 13615  6010 13615 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [10751  2269  6419     2  6419  6419  6419 12166 12166 12166 12166 12166\n","  12166 14125 12166 14125 12166 12166 14125 12166 12166]\n"," [11229 14327 13307 11057 11059  9037 11057 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[ 3651 13390  7866  2016     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7526  2194  3712 12436  9102  3017  9092 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [  745  3271  7717 11059  4624 13615 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 3651 13941  6419     2     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [11229 14572 13307 12811  9102  9037  5771 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 100/743: 1.922603964805603\n","Prediccion:    [[  121  2194  1325  1397  2269  8163 12166     2 12166 12166 12166 14084\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  6472  2194 12249 14422  9102 12853  6010 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 7867  2194  1562 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  8410   342  3017 10256  3017  2860 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664 11433 12217 12818 12393  1325  2295  2269 13895 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","      2]]\n","Ground truth: [[ 7867 11379  9793  1325  2269 14154 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  6472  2194 12249 14422  9102 12853  6010 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 7867  6390  7261 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  8410  2461  8285 10256  3017  2860 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664 11433 12217 11182  8888  9102  2295  2269  1648 12166     2     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]]\n","Pérdida media en el step 200/743: 1.921919345855713\n","Prediccion:    [[ 7867 10932  2269 14156 12796  1945 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 7532  5122  7694  9102  3017  6297 12829 10039  9102  9104 12166     2\n","  12166 12166 12166 12166 12166]\n"," [ 8573 10559 10559 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [10661  5418  8410  3017  2194  1325  3017 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [10751 13366  2269  2932 11436 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]]\n","Ground truth: [[ 7867 10932  2269 14156  4080  1945 12166     2     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7532  5122  9842  1687  3017  6297 12829  3520  9102  9104 12166     2\n","      0     0     0     0     0]\n"," [ 8573  2009 10559 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 9023  5418 13801  2724 11346 10256  7450 12166     2     0     0     0\n","      0     0     0     0     0]\n"," [10751 13366  2269 10609  3500 12166     2     0     0     0     0     0\n","      0     0     0     0     0]]\n","Pérdida media en el step 300/743: 1.9071831703186035\n","Prediccion:    [[  277  8410  9102  7694 11059  6635  4657 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [  745 10039  6458  2906  9102   519  3017  9675  2016     2  2016 12166\n","   2016  2016 12166 12166  2016 12166  2016  2016 12166  2016  2016  2016\n","   2016]\n"," [ 1862  4934 14100   925  9102  2194 11643  2852  8812 10047 12166  7867\n","   2194  2194 12249  2882 12166  1026 12166     2 12166 12166 12166 12166\n","  12166]\n"," [10661 12157  6998  3826  3017  5240  3017   977 12166     2 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]\n"," [ 1664  8410  1861  5865 12166     2 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166]]\n","Ground truth: [[ 9114 10142  9102 14407 11059  6635  4964 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 9120 10039  8679  5992 10941 13071  1639 10255  2016     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1862  8946  9302   925  7867  2194 11643  3961 13046  1430  8812  7867\n","    936  2194 12249  3362  2269  5654 12166     2     0     0     0     0\n","      0]\n"," [10661  7621 13017  4090  3862  5240  3017   977 12166     2     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]\n"," [ 1664  8410  1861   873 12166     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0]]\n","Pérdida media en el step 400/743: 1.891038417816162\n","Prediccion:    [[ 7867  7637  2194 12249  3324  9102 10039 11059  6010 12166 12166     2\n","  12166 12166 12166 12166 12166]\n"," [ 1664 13390  2269   668  9787 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [10661  8410  3977 11059  9755 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166]\n"," [ 7804  1664 14124 14124 10359  2016     2  2016  2016  2016  2016  2016\n","   2016  2016  2016  2016  2016]\n"," [10751  2194 12541  7694   342  9102  9102  3017  9075 10256  5418  9102\n","  12853 12166     2 12166 12166]]\n","Ground truth: [[ 7867  7637  2194 12249   394  7029 10039 13071  6010  3660 12166     2\n","      0     0     0     0     0]\n"," [ 1664 13390  2269 14156  6256 12166     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 5111  8410  8950 11059  1748 12166     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7804  1664 14124  2269  7546  2016     2     0     0     0     0     0\n","      0     0     0     0     0]\n"," [ 7532  2194 12541  7694  6755  8046  9102  3017  9075 10256  3376 10039\n","  12853 12166     2     0     0]]\n","Pérdida media en el step 500/743: 1.8784410953521729\n","Prediccion:    [[ 9120 10039 14134   925  9102   596  3017  2671  1325 10039  2016     2\n","   2016  2016  2016  2016  2016  2016  2016  2016  2016]\n"," [ 7867  2194  8888 12166     2 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [12329  1863  1963  2269 13390 12166     2 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 2981  7867  4202 10039  2016     2  2016  2016  2016  2016  2016  2016\n","   2016  2016  2016  2016  2016  2016  2016  2016  2016]\n"," [  121   521  3017  1932 11059   724 12166     2 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[14286 10039 14134   925  9102   219  3017  9655  1325 10039  2016     2\n","      0     0     0     0     0     0     0     0     0]\n"," [ 7867  9266  8888 12166     2     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 1166  8612   124 14084 13390 12166     2     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [ 2981  7867 10743 10039  2016     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [11229  8492  3017  1932 11059  4571 12166     2     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 600/743: 1.8690061569213867\n","Prediccion:    [[  121  6472  2194 12249 12200 11059  9409 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [12023  2194  3712 10047  7411  5415 10485 10485  5694 12166     2 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194  2269  2269  2635  5913 12166  8729 12166  8729 12166     2\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [  121   521  2009  5240  3017  3017  3423 12166     2 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]\n"," [ 7867  2194  8729  8792 11059     2 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166 12166\n","  12166 12166 12166 12166 12166 12166 12166 12166]]\n","Ground truth: [[  121  6472  2194 12249 12200 11059  9409 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7526  2194  3712 10047  2330  4029   867  2269 14484 12166     2     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7867  6458  2600  2269  3049 14329 11059   835 10256   925 12166     2\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [  121  3042  2009  6041 13999  3017   705 12166     2     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [ 7867 13280  8729  6855 12166     2     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]]\n","Pérdida media en el step 700/743: 1.8575129508972168\n","Pérdida época 9: 1.8531184196472168\n"]}],"source":["func_perdida = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True,\n","    ignore_class=0, # Ignorar padding\n",")\n","optimizador = keras.optimizers.Adam(\n","    learning_rate=0.0001,\n","    beta_1=0.9,\n","    beta_2=0.98,\n","    epsilon=1e-9\n",")\n","\n","epocas = 10\n","tam_batch = 128\n","\n","for epoca in range(epocas):\n","    print(f'Comienzo de epoca {epoca}')\n","    train_loader = generador_batch(X_train, Y_train, batch_size=tam_batch)\n","\n","    epoca_perdidas = []\n","\n","    for step, (x, entrada_decoder, y) in enumerate(train_loader):\n","\n","        # Objeto GradientTape para grabar las operaciones durante la pasada\n","        # forward y hacer la auto-diferenciacion\n","        with tf.GradientTape() as tape:\n","            # Forward del modelo Transformer\n","            logits = modelo(x, entrada_decoder)  # Logits for this minibatch\n","            # Calcular la perdida para el mini-batch\n","            loss_value = func_perdida(y, logits)\n","\n","            epoca_perdidas.append(loss_value)\n","\n","            if step % 100 == 0:\n","              res = tf.argmax(logits, axis=-1).numpy()\n","              print(f'Prediccion:    {res[:5]}')\n","              print(f'Ground truth: {y[:5]}')\n","\n","        # Llamada a método .gradient para calcular los gradientes de los pesos\n","        # con respecto a la pérdida obtenida\n","        grads = tape.gradient(loss_value, modelo.trainable_weights)\n","\n","        # Aplicar un paso del descenso del gradiente,\n","        # actualizando los pesos del modelo\n","        optimizador.apply_gradients(zip(grads, modelo.trainable_weights))\n","\n","        # Log cada 100 steps.\n","        if step % 100 == 0:\n","            print(f'Pérdida media en el step {step}/{len(X_train)//tam_batch}: {sum(epoca_perdidas)/len(epoca_perdidas)}')\n","\n","    print(f'Pérdida época {epoca}: {sum(epoca_perdidas)/len(epoca_perdidas)}')\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"eQrIwfwCcC_2","executionInfo":{"status":"ok","timestamp":1690991935255,"user_tz":-120,"elapsed":19,"user":{"displayName":"Carlos Garrido","userId":"04511245114116980775"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ip4llu0_qc0j"},"source":["Pérdida y optimizador del modelo"]},{"cell_type":"markdown","metadata":{"id":"jiS3QPf4qc0j"},"source":["Entrenamiento de 15 épocas"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"4cr_cbarNDsm","executionInfo":{"status":"ok","timestamp":1690991935256,"user_tz":-120,"elapsed":17,"user":{"displayName":"Carlos Garrido","userId":"04511245114116980775"}}},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","\n","def decodificacion_voraz(modelo, src, src_mask, max_len, tgt_w2i, tgt_i2w):\n","    # Codificación\n","    src_cod = modelo.codificar(src, src_mask)\n","\n","    # Decodificación\n","    tgt_token = tf.constant([[tgt_w2i['SOS']]], dtype=tf.int64)\n","\n","    print(f'Tgt token shape: {tgt_token.shape}')\n","\n","    tgt_pred_decod = []\n","    for i in range(max_len):\n","        # Predicción del modelo\n","        tgt_mask = modelo.crear_mascara(tgt_token, tgt_token)[1]\n","        tgt_pred = modelo.decodificar(tgt_token, src_cod, tgt_mask)\n","        tgt_pred = tgt_pred[:, -1, :]  # Último token\n","\n","        # Nos quedamos con el token más probable\n","        print(f'{tf.argmax(tgt_pred, axis=-1).numpy()[0]}')\n","        tgt_pred = tf.argmax(tgt_pred, axis=-1).numpy()[0]\n","        tgt_pred_decod.append(tgt_i2w[tgt_pred])\n","\n","        print(f'token predicho: {tgt_pred}')\n","        print(f'secuencia: {tgt_pred_decod}')\n","\n","        # Preparamos la nueva entrada del decoder\n","        tgt_token = np.hstack((tgt_token, np.array([[tgt_pred]])))\n","\n","        # Comprobamos si se ha predicho el token de fin de secuencia\n","        if tgt_pred_decod[-1] == 'EOS':\n","            break\n","\n","    return tgt_pred_decod\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"z9I9Lp2GMAvl","executionInfo":{"status":"ok","timestamp":1690991935258,"user_tz":-120,"elapsed":19,"user":{"displayName":"Carlos Garrido","userId":"04511245114116980775"}}},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","\n","def traducir(modelo, src_frase, src_w2i, tgt_w2i, tgt_i2w):\n","    # Codificamos la secuencia de entrada\n","    src_cod = codificar([src_frase], src_w2i)\n","    src_cod = tf.convert_to_tensor(src_cod, dtype=tf.int64)\n","    # src_cod = tf.expand_dims(src_cod, axis=0)  # Agregamos dimensión de batch [1, sec_len]\n","\n","    # Máscara de ceros para el source (dejamos ver todo)\n","    src_mask = tf.zeros((src_cod.shape[1], src_cod.shape[1]))\n","\n","    # Permitimos hasta 5 tokens más en la traducción\n","    max_len = src_cod.shape[1] + 5\n","\n","    # Iniciamos la traducción\n","    tgt_pred_decod = decodificacion_voraz(modelo, src_cod, src_mask, max_len, tgt_w2i, tgt_i2w)\n","\n","    # Quitamos los tokens de inicio y fin de secuencia\n","    tgt_pred_decod = [t for t in tgt_pred_decod if t not in ['SOS', 'EOS']]\n","    return ' '.join(tgt_pred_decod)\n"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"FThXBKAS0N-L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690992410425,"user_tz":-120,"elapsed":4082,"user":{"displayName":"Carlos Garrido","userId":"04511245114116980775"}},"outputId":"3deae7b1-295e-4e09-de60-44a6296491c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tgt token shape: (1, 1)\n","7867\n","token predicho: 7867\n","secuencia: ['I']\n","9883\n","token predicho: 9883\n","secuencia: ['I', 'hope']\n","10039\n","token predicho: 10039\n","secuencia: ['I', 'hope', 'you']\n","2194\n","token predicho: 2194\n","secuencia: ['I', 'hope', 'you', \"'\"]\n","12541\n","token predicho: 12541\n","secuencia: ['I', 'hope', 'you', \"'\", 'll']\n","7694\n","token predicho: 7694\n","secuencia: ['I', 'hope', 'you', \"'\", 'll', 'be']\n","8743\n","token predicho: 8743\n","secuencia: ['I', 'hope', 'you', \"'\", 'll', 'be', 'able']\n","9102\n","token predicho: 9102\n","secuencia: ['I', 'hope', 'you', \"'\", 'll', 'be', 'able', 'to']\n","2882\n","token predicho: 2882\n","secuencia: ['I', 'hope', 'you', \"'\", 'll', 'be', 'able', 'to', 'get']\n","3017\n","token predicho: 3017\n","secuencia: ['I', 'hope', 'you', \"'\", 'll', 'be', 'able', 'to', 'get', 'the']\n","6729\n","token predicho: 6729\n","secuencia: ['I', 'hope', 'you', \"'\", 'll', 'be', 'able', 'to', 'get', 'the', 'same']\n","1342\n","token predicho: 1342\n","secuencia: ['I', 'hope', 'you', \"'\", 'll', 'be', 'able', 'to', 'get', 'the', 'same', 'language']\n","12166\n","token predicho: 12166\n","secuencia: ['I', 'hope', 'you', \"'\", 'll', 'be', 'able', 'to', 'get', 'the', 'same', 'language', '.']\n","2\n","token predicho: 2\n","secuencia: ['I', 'hope', 'you', \"'\", 'll', 'be', 'able', 'to', 'get', 'the', 'same', 'language', '.', 'EOS']\n","Original: Espero que te haya gustado el libro\n","Traducción: I hope you ' ll be able to get the same language .\n"]}],"source":["src_frase = 'Espero que te haya gustado el libro'\n","tgt_frase = traducir(\n","    modelo,\n","    src_frase,\n","    X_w2i,\n","    Y_w2i, Y_i2w,\n",")\n","print(f'Original: {src_frase}\\nTraducción: {tgt_frase}')"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"ltSReCTSPlXY","executionInfo":{"status":"ok","timestamp":1690991937348,"user_tz":-120,"elapsed":4,"user":{"displayName":"Carlos Garrido","userId":"04511245114116980775"}}},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}