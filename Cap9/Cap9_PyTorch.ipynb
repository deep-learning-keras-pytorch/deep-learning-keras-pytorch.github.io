{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6x6Q8DWqc0a"
      },
      "source": [
        "# **Capítulo 9: Procesamiento de lenguaje natural**\n",
        "\n",
        "## Traducción automática de texto: de español a inglés"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7JjoFCHqc0c"
      },
      "source": [
        "Descargamos los datos del caso práctico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8IvToV9qc0d"
      },
      "outputs": [],
      "source": [
        "!wget -q http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
        "!unzip -q spa-eng.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ9CTBBbqc0e"
      },
      "source": [
        "Carga del conjunto de datos de traducción automática"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OcSu1zGqc0e",
        "outputId": "f283183f-75f1-4ca5-c886-06855428b3ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de pares de oraciones: 118964\n",
            "Posible entrada: Estoy levantado.\n",
            "Posible salida: I'm up.\n"
          ]
        }
      ],
      "source": [
        "def cargar_datos():\n",
        "    with open('spa-eng/spa.txt', 'r') as f:\n",
        "        lineas = f.read().splitlines()\n",
        "    pares = [linea.split('\\t') for linea in lineas]\n",
        "    esp = [par[1] for par in pares]\n",
        "    ing = [par[0] for par in pares]\n",
        "    return esp, ing\n",
        "\n",
        "src, tgt = cargar_datos()\n",
        "print(f'Número de pares de oraciones: {len(src)}')\n",
        "print(f'Posible entrada: {src[50]}')\n",
        "print(f'Posible salida: {tgt[50]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNp97-kxqc0f"
      },
      "source": [
        "Creación de los vocabularios de español e inglés"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-D5tKiYqc0f",
        "outputId": "b6c24751-f675-4b67-cfc4-63d01b2ee686"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño del vocabulario de español: 28993\n",
            "Tamaño del vocabulario de inglés: 14779\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def crear_vocab(frases):\n",
        "    # Obtenemos el vocabulario\n",
        "    vocab = set()\n",
        "    for f in frases:\n",
        "        # Expresión regular para separar palabras\n",
        "        # manteniendo signos de puntuación\n",
        "        vocab.update(re.findall(r'\\w+|[^\\w\\s]', f))\n",
        "\n",
        "    # Creamos los diccionarios\n",
        "    w2i = {w: i+4 for i, w in enumerate(vocab)}\n",
        "    w2i['PAD'] = 0\n",
        "    w2i['SOS'] = 1\n",
        "    w2i['EOS'] = 2\n",
        "    w2i['UNK'] = 3\n",
        "    i2w = {i: w for w, i in w2i.items()}\n",
        "\n",
        "    return w2i, i2w\n",
        "\n",
        "src_w2i, src_i2w = crear_vocab(src)\n",
        "tgt_w2i, tgt_i2w = crear_vocab(tgt)\n",
        "print(f'Tamaño del vocabulario de español: {len(src_w2i)}')\n",
        "print(f'Tamaño del vocabulario de inglés: {len(tgt_w2i)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEJjeh_aqc0g"
      },
      "source": [
        "Codificación de las secuencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iu4FiMhwqc0g"
      },
      "outputs": [],
      "source": [
        "def codificar_secuencias(secs, w2i):\n",
        "    secs_cod = []\n",
        "    for s in secs:\n",
        "        s_cod = [w2i[w] for w in re.findall(r'\\w+|[^\\w\\s]', s)]\n",
        "        s_cod = [w2i['SOS']] + s_cod + [w2i['EOS']]\n",
        "        secs_cod.append(s_cod)\n",
        "    return secs_cod\n",
        "\n",
        "src_cod = codificar_secuencias(src, src_w2i)\n",
        "tgt_cod = codificar_secuencias(tgt, tgt_w2i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrCs6ZxPqc0h"
      },
      "source": [
        "División del conjunto de datos en entrenamiento y test (80-20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hALis3qZqc0h",
        "outputId": "47d4f9c7-7ebd-47d3-be22-42b4a15f7ffe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¡Particiones realizadas!\n",
            "Tamaño del conjunto de entrenamiento: 95171\n",
            "Tamaño del conjunto de test: 23793\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "src_train, src_test, tgt_train, tgt_test = train_test_split(src_cod, tgt_cod,\\\n",
        "                                                    test_size=0.2,\\\n",
        "                                                    random_state=42)\n",
        "print('¡Particiones realizadas!')\n",
        "print(f'Tamaño del conjunto de entrenamiento: {len(src_train)}')\n",
        "print(f'Tamaño del conjunto de test: {len(src_test)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAU5pnQzqc0h"
      },
      "source": [
        "Preprocesado de los datos de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbazgppAqc0h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def preproceso_batch(X, Y):\n",
        "    max_len_X = max([len(x) for x in X])\n",
        "    max_len_Y = max([len(y) for y in Y])\n",
        "\n",
        "    encoder_input = torch.zeros(len(X), max_len_X)\n",
        "    decoder_input = torch.zeros(len(Y), max_len_Y)\n",
        "    salida = torch.zeros(len(Y), max_len_Y)\n",
        "\n",
        "    for i, s in enumerate(X):\n",
        "        # Sec. completa con relleno para el encoder (frase a traducir)\n",
        "        encoder_input[i, :len(s)] = torch.tensor(s)\n",
        "\n",
        "    for i, s in enumerate(Y):\n",
        "        # Sec. sin el \"EOS\" con relleno para el decoder (traducción)\n",
        "        decoder_input[i, :len(s)-1] = torch.tensor(s[:-1])\n",
        "        # Sec. sin el \"SOS\" con relleno para la salida (traducción)\n",
        "        salida[i, :len(s)-1] = torch.tensor(s[1:])\n",
        "\n",
        "    return encoder_input.long(), decoder_input.long(), salida.long()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz_FKFRXqc0h"
      },
      "source": [
        "Creación de un generador de batches o data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faJzMLscqc0h",
        "outputId": "16396c19-cdd5-4efc-aff0-7773407def30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrada al encoder: ['SOS', 'No', 'tengo', 'otra', 'opción', 'en', 'absoluto', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "Entrada al decoder: ['SOS', 'I', 'have', 'no', 'choice', 'at', 'all', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "Salida del decoder: ['I', 'have', 'no', 'choice', 'at', 'all', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "def generador_batch(X, Y, batch_size):\n",
        "    idx = 0\n",
        "    while True:\n",
        "        bx = X[idx:idx+batch_size]\n",
        "        by = Y[idx:idx+batch_size]\n",
        "\n",
        "        yield preproceso_batch(bx, by)\n",
        "\n",
        "        idx = (idx + batch_size) % len(X)\n",
        "        if idx == 0:\n",
        "            X, Y = shuffle(X, Y, random_state=42)\n",
        "\n",
        "batch_size = 128\n",
        "train_loader = generador_batch(src_train, tgt_train, batch_size=batch_size)\n",
        "be, bd, bs = next(train_loader)\n",
        "print(f'Entrada al encoder: {[src_i2w[w.item()]for w in be[0]]}')\n",
        "print(f'Entrada al decoder: {[tgt_i2w[w.item()]for w in bd[0]]}')\n",
        "print(f'Salida del decoder: {[tgt_i2w[w.item()]for w in bs[0]]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc2_j1vZqc0i"
      },
      "source": [
        "Definición de la capa de codificación de posición"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AowCb_3Gqc0i"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_len, emb_dim, dropout=0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pos = torch.arange(max_len).unsqueeze(1)\n",
        "        # 2 * torch.arange(emb_dim // 2)  == torch.arange(0, emb_dim, 2)\n",
        "        den = torch.pow(10000, torch.arange(0, emb_dim, 2) / emb_dim)\n",
        "        pe = torch.zeros(1, max_len, emb_dim)\n",
        "        pe[0, :, 0::2] = torch.sin(pos / den)\n",
        "        pe[0, :, 1::2] = torch.cos(pos / den)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape = [batch_size, sec_len, emb_dim]\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVxan_-dqc0i"
      },
      "source": [
        "Definición del modelo Transformer a usar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vT1TRIVqc0i"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 max_len,\n",
        "                 emb_dim,\n",
        "                 num_encoder_layers,\n",
        "                 num_decoder_layers,\n",
        "                 nhead,\n",
        "                 src_vocab_size,\n",
        "                 tgt_vocab_size,\n",
        "                 dim_feedforward,\n",
        "                 dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        # Capas de embedding + codificación de posición\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, emb_dim)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, emb_dim)\n",
        "        self.pe = PositionalEncoding(max_len, emb_dim, dropout)\n",
        "\n",
        "        # Transformer\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=emb_dim,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Capa de salida\n",
        "        self.salida = nn.Linear(emb_dim, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # Máscara de padding\n",
        "        src_mask, tgt_mask,\\\n",
        "            src_pad_mask, tgt_pad_mask = self.crear_mascara(src, tgt)\n",
        "        # Embedding + codificación de posición\n",
        "        src_emb = self.pe(self.src_embedding(src))\n",
        "        tgt_emb = self.pe(self.tgt_embedding(tgt))\n",
        "        # Transformer\n",
        "        tgt_pred = self.transformer(src_emb, tgt_emb,\n",
        "                                    src_mask=src_mask,\n",
        "                                    tgt_mask=tgt_mask,\n",
        "                                    src_key_padding_mask=src_pad_mask,\n",
        "                                    tgt_key_padding_mask=tgt_pad_mask,\n",
        "                                    memory_key_padding_mask=src_pad_mask)\n",
        "        # Salida (clasificación)\n",
        "        tgt_pred = self.salida(tgt_pred)\n",
        "        return tgt_pred\n",
        "\n",
        "    def codificar(self, src, src_mask):\n",
        "        # Embedding + codificación de posición\n",
        "        src_emb = self.pe(self.src_embedding(src))\n",
        "        # Transformer encoder\n",
        "        return self.transformer.encoder(src_emb, src_mask)\n",
        "\n",
        "    def decodificar(self, tgt, memory, tgt_mask):\n",
        "        # Embedding + codificación de posición\n",
        "        tgt_emb = self.pe(self.tgt_embedding(tgt))\n",
        "        # Transformer decoder\n",
        "        tgt_pred = self.transformer.decoder(tgt_emb, memory, tgt_mask)\n",
        "        # Salida (clasificación)\n",
        "        tgt_pred = self.salida(tgt_pred)\n",
        "        return tgt_pred\n",
        "\n",
        "    def crear_mascara(self, src, tgt):\n",
        "        # src/tgt.shape = [batch_size, src/tgt_sec_len, emb_dim]\n",
        "        src_sec_len = src.shape[1]\n",
        "        tgt_sec_len = tgt.shape[1]\n",
        "\n",
        "        # Máscara de ceros (dejamos ver todo)\n",
        "        src_mask = torch.zeros((src_sec_len, src_sec_len),\n",
        "                               device=src.device)\n",
        "        # Máscara triangular superior para el target\n",
        "        tgt_mask = self.transformer.generate_square_subsequent_mask(\n",
        "            tgt_sec_len, tgt.device)\n",
        "\n",
        "        # 0 == \"PAD\"\n",
        "        src_pad_mask = (src == 0)\n",
        "        tgt_pad_mask = (tgt == 0)\n",
        "\n",
        "        return src_mask, tgt_mask, src_pad_mask, tgt_pad_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb_HiGK1qc0i"
      },
      "source": [
        "Instancia del modelo Transformer a usar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Su8TOxONqc0i"
      },
      "outputs": [],
      "source": [
        "# Comprobamos si tenemos GPU\n",
        "dispositivo = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Calculamos la máxima longitud de las frases\n",
        "max_len = max([len(x) for x in src + tgt])\n",
        "\n",
        "# Hiperparámetros del artículo del Transformer\n",
        "modelo = Transformer(\n",
        "    max_len=max_len,\n",
        "    emb_dim=512,\n",
        "    num_encoder_layers=6,\n",
        "    num_decoder_layers=6,\n",
        "    nhead=8,\n",
        "    src_vocab_size=len(src_w2i),\n",
        "    tgt_vocab_size=len(tgt_w2i),\n",
        "    dim_feedforward=2048,\n",
        "    dropout=0.1\n",
        ").to(dispositivo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip4llu0_qc0j"
      },
      "source": [
        "Pérdida y optimizador del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jopioEe5qc0j"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Pérdida y optimizador\n",
        "func_perdida = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizador = optim.Adam(modelo.parameters(),\n",
        "                         lr=0.0001,\n",
        "                         betas=(0.9, 0.98),\n",
        "                         eps=1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiS3QPf4qc0j"
      },
      "source": [
        "Entrenamiento de 20 épocas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDNOGZo5qc0j",
        "outputId": "b224f5e6-eb7d-4340-a36e-1b8763a9ae10"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Época 1/20,          pérdida=4.0026,\n",
            "Época 2/20,          pérdida=2.8578,\n",
            "Época 3/20,          pérdida=2.3689,\n",
            "Época 4/20,          pérdida=2.0082,\n",
            "Época 5/20,          pérdida=1.7391,\n",
            "Época 6/20,          pérdida=1.5368,\n",
            "Época 7/20,          pérdida=1.3748,\n",
            "Época 8/20,          pérdida=1.2430,\n",
            "Época 9/20,          pérdida=1.1319,\n",
            "Época 10/20,          pérdida=1.0354,\n",
            "Época 11/20,          pérdida=0.9523,\n",
            "Época 12/20,          pérdida=0.8774,\n",
            "Época 13/20,          pérdida=0.8111,\n",
            "Época 14/20,          pérdida=0.7498,\n",
            "Época 15/20,          pérdida=0.6945,\n",
            "Época 16/20,          pérdida=0.6454,\n",
            "Época 17/20,          pérdida=0.5984,\n",
            "Época 18/20,          pérdida=0.5581,\n",
            "Época 19/20,          pérdida=0.5195,\n",
            "Época 20/20,          pérdida=0.4840,\n",
            "Fin del entrenamiento.\n"
          ]
        }
      ],
      "source": [
        "modelo.train()\n",
        "epocas = 20\n",
        "for epoca in range(epocas):\n",
        "    epoca_perdidas = []\n",
        "    for _ in range(len(src_train) // batch_size):\n",
        "        # Obtenemos un lote de datos\n",
        "        be, bd, bs = next(train_loader)\n",
        "        # Enviamos los datos al dispositivo (GPU o CPU)\n",
        "        be = be.to(dispositivo)\n",
        "        bd = bd.to(dispositivo)\n",
        "        bs = bs.to(dispositivo)\n",
        "        # Calculamos la pérdida y\n",
        "        # actualizamos los parámetros\n",
        "        optimizador.zero_grad()\n",
        "        bs_pred = modelo(be, bd)\n",
        "        perdida = func_perdida(bs_pred.permute(0, 2, 1), bs)\n",
        "        perdida.backward()\n",
        "        optimizador.step()\n",
        "        # Guardamos la pérdida\n",
        "        epoca_perdidas.append(perdida.detach().cpu().item())\n",
        "    # Pérdida promedio de la época\n",
        "    perdida_epoca = sum(epoca_perdidas) / len(epoca_perdidas)\n",
        "    print(f'Época {epoca+1}/{epocas}, pérdida={perdida_epoca:.4f}')\n",
        "print('Fin del entrenamiento.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoxF8e5jrEaP"
      },
      "outputs": [],
      "source": [
        "def decodificacion_voraz(modelo,\n",
        "                         src, src_mask,\n",
        "                         max_len,\n",
        "                         tgt_w2i, tgt_i2w,\n",
        "                         dispositivo):\n",
        "    # Codificación\n",
        "    src_cod = modelo.codificar(src, src_mask)\n",
        "\n",
        "    # Decodificación\n",
        "    tgt_token = torch.tensor([tgt_w2i['SOS']]).unsqueeze(0).long()\n",
        "    tgt_token = tgt_token.to(dispositivo)\n",
        "\n",
        "    tgt_pred_decod = []\n",
        "    for i in range(max_len):\n",
        "        # Predicción del modelo\n",
        "        tgt_mask = modelo.transformer.generate_square_subsequent_mask(\n",
        "            tgt_token.size(1), dispositivo)\n",
        "        tgt_pred = modelo.decodificar(tgt_token, src_cod, tgt_mask)\n",
        "        tgt_pred = tgt_pred[0, -1, :] # Último token\n",
        "\n",
        "        # Nos quedamos con el token más probable\n",
        "        tgt_pred = tgt_pred.argmax(dim=-1).item()\n",
        "        tgt_pred_decod.append(tgt_i2w[tgt_pred])\n",
        "\n",
        "        # Preparamos la nueva entrada del decoder\n",
        "        tgt_token = torch.cat((tgt_token,\n",
        "                               torch.full((1, 1),\n",
        "                                          tgt_pred,\n",
        "                                          device=dispositivo)),\n",
        "                               dim=1)\n",
        "\n",
        "        # Comprobamos si se ha predicho el token de\n",
        "        # fin de secuencia\n",
        "        if tgt_pred_decod[-1] == 'EOS':\n",
        "            break\n",
        "\n",
        "    return tgt_pred_decod"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sj5NN_ijxNPO"
      },
      "outputs": [],
      "source": [
        "def traducir(modelo,\n",
        "             src_frase, src_w2i,\n",
        "             tgt_w2i, tgt_i2w,\n",
        "             dispositivo):\n",
        "    # Codificamos la secuencia de entrada\n",
        "    src_cod = codificar_secuencias([src_frase], src_w2i)\n",
        "    src_cod = torch.tensor(src_cod).long().to(dispositivo) # [1, sec_len]\n",
        "    # Máscara de ceros para el soruce (dejamos ver todo)\n",
        "    src_mask = torch.zeros((src_cod.size(1), src_cod.size(1)),\n",
        "                           device=dispositivo)\n",
        "\n",
        "    # Permitimos hasta 5 tokens más en la traducción\n",
        "    max_len = src_cod.size(1) + 5\n",
        "\n",
        "    # Iniciamos la traducción\n",
        "    modelo.eval()\n",
        "    with torch.no_grad():\n",
        "        tgt_pred_decod = decodificacion_voraz(\n",
        "            modelo,\n",
        "            src_cod,\n",
        "            src_mask,\n",
        "            max_len,\n",
        "            tgt_w2i,\n",
        "            tgt_i2w,\n",
        "            dispositivo)\n",
        "    # Quitamos los tokens de inicio y fin de secuencia\n",
        "    tgt_pred_decod = [t for t in tgt_pred_decod if t not in ['SOS', 'EOS']]\n",
        "    return ' '.join(tgt_pred_decod)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FThXBKAS0N-L",
        "outputId": "a6d8b860-37ef-4dc2-909a-67da6326a00d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: Espero que te haya gustado el caso de estudio\n",
            "Traducción: I hope you ' ll like the study of the study .\n"
          ]
        }
      ],
      "source": [
        "src_frase = 'Espero que te haya gustado el caso de estudio'\n",
        "tgt_frase = traducir(\n",
        "    modelo,\n",
        "    src_frase,\n",
        "    src_w2i,\n",
        "    tgt_w2i, tgt_i2w,\n",
        "    dispositivo\n",
        ")\n",
        "print(f'Original: {src_frase}\\nTraducción: {tgt_frase}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
