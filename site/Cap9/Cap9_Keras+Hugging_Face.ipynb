{"cells":[{"cell_type":"markdown","source":["# **Capítulo 9: Procesamiento de lenguaje natural**\n","\n","## Traducción automática de texto: de español a inglés"],"metadata":{"id":"c7-gsPiMeTtm"}},{"cell_type":"markdown","source":["## Traducción mediante un modelo de Transformer"],"metadata":{"id":"-Yq96oY8gKJL"}},{"cell_type":"markdown","source":["Instalación de los recursos necesarios para el caso práctico"],"metadata":{"id":"cg7xGrOKemIX"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"avTpOS_n2l1m","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0f8b664a-9cb9-44d5-ea35-7df4620bfabf","executionInfo":{"status":"ok","timestamp":1691319877771,"user_tz":-120,"elapsed":80318,"user":{"displayName":"Antoniorv6","userId":"12791290898792286358"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m851.9/851.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for keras-nlp (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!wget -q http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n","!unzip -q spa-eng.zip\n","!pip install -q git+https://github.com/keras-team/keras-nlp.git --upgrade"]},{"cell_type":"markdown","source":["Importación de keras_nlp"],"metadata":{"id":"a3TfQbhqeqwd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"AMg7dHAqAI36","colab":{"base_uri":"https://localhost:8080/"},"outputId":"92ef1493-267a-4550-98b0-988caaf97d13"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using TensorFlow backend\n"]}],"source":["import keras_nlp"]},{"cell_type":"markdown","source":["Cargado de datos"],"metadata":{"id":"aJeE8ireeusZ"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7RqH_uFH_9KF","outputId":"b56eca0f-db9a-473f-a8e3-9d87da67b217","executionInfo":{"status":"ok","timestamp":1691319913337,"user_tz":-120,"elapsed":1054,"user":{"displayName":"Antoniorv6","userId":"12791290898792286358"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Número de pares de oraciones: 118964\n","Posible entrada: Estoy levantado.\n","Posible salida: I'm up.\n"]}],"source":["def cargar_datos():\n","    with open('spa-eng/spa.txt', 'r') as f:\n","        lineas = f.read().splitlines()\n","    pares = [linea.split('\\t') for linea in lineas]\n","    esp = [par[1] for par in pares]\n","    ing = [par[0] for par in pares]\n","    return esp, ing\n","\n","X, Y = cargar_datos()\n","print(f'Número de pares de oraciones: {len(X)}')\n","print(f'Posible entrada: {X[50]}')\n","print(f'Posible salida: {Y[50]}')\n"]},{"cell_type":"markdown","source":["Creación del vocabulario"],"metadata":{"id":"58sOcImEewhl"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HWsTQz9CBkSH","outputId":"b8b5010f-7313-43ec-d6fe-924d010525e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño del vocabulario de español: 28993\n","Tamaño del vocabulario de inglés: 14779\n"]}],"source":["import re\n","\n","def crear_vocab(frases):\n","   # Obtenemos el vocabulario\n","   vocab = set()\n","   for f in frases:\n","       # Expresión regular para separar palabras\n","       # manteniendo signos de puntuación\n","       vocab.update(re.findall(r'\\w+|[^\\w\\s]', f))\n","\n","   # Creamos los diccionarios\n","   w2i = {w: i+4 for i, w in enumerate(vocab)}\n","   w2i['PAD'] = 0\n","   w2i['SOS'] = 1\n","   w2i['EOS'] = 2\n","   w2i['UNK'] = 3\n","   i2w = {i: w for w, i in w2i.items()}\n","\n","   return w2i, i2w\n","\n","X_w2i, X_i2w = crear_vocab(X)\n","Y_w2i, Y_i2w = crear_vocab(Y)\n","print(f'Tamaño del vocabulario de español: {len(X_w2i)}')\n","print(f'Tamaño del vocabulario de inglés: {len(Y_w2i)}')"]},{"cell_type":"markdown","source":["Codificación de las secuencias con el vocabulario creado"],"metadata":{"id":"0g1iFDMLeyjp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"F-rWy6miBoN4"},"outputs":[],"source":["def codificar(secs, w2i):\n","    secs_cod = []\n","    for s in secs:\n","        s_cod = [w2i[w] for w in re.findall(r'\\w+|[^\\w\\s]', s)]\n","        s_cod = [w2i['SOS']] + s_cod + [w2i['EOS']]\n","        secs_cod.append(s_cod)\n","    return secs_cod\n","\n","X_cod = codificar(X, X_w2i)\n","Y_cod = codificar(Y, Y_w2i)"]},{"cell_type":"markdown","source":["División del conjunto de datos en entrenamiento y test (80-20)"],"metadata":{"id":"Ur5L-Tiee2r1"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5-4RnZW4BCtC","outputId":"f6aabb55-764e-4409-8b24-1ed06676e972"},"outputs":[{"output_type":"stream","name":"stdout","text":["¡Particiones realizadas!\n","Tamaño del conjunto de entrenamiento: 95171\n","Tamaño del conjunto de test: 23793\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X_cod, Y_cod,\\\n","                                                    test_size=0.2,\\\n","                                                    random_state=42)\n","print('¡Particiones realizadas!')\n","print(f'Tamaño del conjunto de entrenamiento: {len(X_train)}')\n","print(f'Tamaño del conjunto de test: {len(X_test)}')"]},{"cell_type":"markdown","source":["Preprocesado de los datos de entrenamiento"],"metadata":{"id":"cd52CEore479"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7j-NDcbBRwe"},"outputs":[],"source":["import numpy as np\n","\n","def preproceso_batch(X, Y):\n","   max_len_X = max([len(x) for x in X])\n","   max_len_Y = max([len(y) for y in Y])\n","\n","   encoder_input = np.zeros((len(X), max_len_X))\n","   decoder_input = np.zeros((len(Y), max_len_Y))\n","   salida = np.zeros((len(Y), max_len_Y))\n","\n","   for i, s in enumerate(X):\n","       # Sec. completa con relleno para el encoder (frase a traducir)\n","       encoder_input[i, :len(s)] = np.array(s)\n","\n","   for i, s in enumerate(Y):\n","       # Sec. sin el \"EOS\" con relleno para el decoder (traducción)\n","       decoder_input[i, :len(s)-1] = np.array(s[:-1])\n","       # Sec. sin el \"SOS\" con relleno para la salida (traducción)\n","       salida[i, :len(s)-1] = np.array(s[1:])\n","\n","   src_pad_mask = (encoder_input == 0)\n","   tgt_pad_mask = (decoder_input == 0)\n","\n","   encoder_input = encoder_input.astype(np.int64)\n","   decoder_input = decoder_input.astype(np.int64)\n","   salida = salida.astype(np.int64)\n","\n","   return [encoder_input, decoder_input, src_pad_mask, tgt_pad_mask], salida"]},{"cell_type":"markdown","source":["Creación de un generador de batches"],"metadata":{"id":"UBtMFRzAe90l"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FoSzCofFBfwy","outputId":"b7cd9033-81aa-419d-e063-363958192511"},"outputs":[{"output_type":"stream","name":"stdout","text":["Entrada al encoder: ['SOS', 'No', 'tengo', 'otra', 'opción', 'en', 'absoluto', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","Entrada al decoder: ['SOS', 'I', 'have', 'no', 'choice', 'at', 'all', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","Salida del decoder: ['I', 'have', 'no', 'choice', 'at', 'all', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","Mascara del encoder: [False False False False False False False False False  True  True  True\n","  True  True  True  True  True  True]\n","Mascara del decoder: [False False False False False False False False  True  True  True  True\n","  True  True  True  True  True  True  True  True  True]\n"]}],"source":["from sklearn.utils import shuffle\n","\n","def generador_batch(X, Y, batch_size):\n","    idx = 0\n","    while True:\n","        bx = X[idx:idx+batch_size]\n","        by = Y[idx:idx+batch_size]\n","\n","        yield preproceso_batch(bx, by)\n","\n","        idx = (idx + batch_size) % len(X)\n","\n","\n","batch_size = 128\n","train_loader = generador_batch(X_train, Y_train, batch_size=batch_size)\n","[be, bd, sp, tp], bs = next(train_loader)\n","print(f'Entrada al encoder: {[X_i2w[w.item()]for w in be[0]]}')\n","print(f'Entrada al decoder: {[Y_i2w[w.item()]for w in bd[0]]}')\n","print(f'Salida del decoder: {[Y_i2w[w.item()]for w in bs[0]]}')\n","print(f'Mascara del encoder: {sp[0]}')\n","print(f'Mascara del decoder: {tp[0]}')"]},{"cell_type":"markdown","source":["Creación del Positional Encoding"],"metadata":{"id":"cD6hskGZfX6c"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yrobS_kMwYgc"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","\n","class PositionalEncoding(tf.keras.layers.Layer):\n","    def __init__(self, max_len, emb_dim, dropout=0.1):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = tf.keras.layers.Dropout(dropout)\n","\n","        pos = np.arange(max_len).reshape(-1, 1)\n","        den = np.power(10000, np.arange(0, emb_dim, 2) / emb_dim)\n","        pe = np.zeros((1, max_len, emb_dim))\n","        pe[0, :, 0::2] = np.sin(pos / den)\n","        pe[0, :, 1::2] = np.cos(pos / den)\n","        self.pe = tf.constant(pe, dtype=tf.float32)\n","\n","    def call(self, x):\n","        # x.shape = [batch_size, sec_len, emb_dim]\n","        x = x + self.pe[:, :tf.shape(x)[1], :]\n","        return self.dropout(x)"]},{"cell_type":"markdown","source":["Creación del modelo de Transformer"],"metadata":{"id":"UpoH-P9wfaHp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HEkc5JID1v9J"},"outputs":[],"source":["def crear_transformer(max_long,\n","                      emb_dim,\n","                      num_enc_capas,\n","                      num_dec_capas,\n","                      ncabezas,\n","                      src_vocab_tam,\n","                      tgt_vocab_tam,\n","                      dim_mlp,\n","                      dropout=0.1):\n","\n","  # Definicion del encoder\n","  enc_entradas = tf.keras.Input(shape=(None,),\n","                                dtype=\"int64\",\n","                                name=\"enc_entradas\")\n","  mask_entradas_encoder = tf.keras.Input(shape=(None,),\n","                                         dtype=\"int64\",\n","                                         name=\"mask_entradas_encoder\")\n","\n","  enc_salidas = tf.keras.layers.Embedding(src_vocab_tam, emb_dim)(enc_entradas)\n","  enc_salidas = PositionalEncoding(max_long, emb_dim, 0.1)(enc_salidas)\n","\n","  for i in range(num_enc_capas):\n","       enc_salidas = keras_nlp.layers.TransformerEncoder(\n","           intermediate_dim=dim_mlp,\n","           num_heads=ncabezas,\n","           dropout=dropout,\n","           activation=\"relu\",\n","           name=None)(enc_salidas, padding_mask=mask_entradas_encoder)\n","\n","  # Definicion del decoder\n","  dec_entradas = tf.keras.Input(shape=(None,), dtype=\"int64\",\n","                      name=\"dec_entradas\")\n","  enc_seq_entradas = tf.keras.Input(shape=(None, emb_dim),\n","                          name=\"dec_state_entradas\")\n","\n","  mask_entradas_decoder = tf.keras.Input(shape=(None,),\n","                                         dtype=\"int64\",\n","                                         name=\"mask_entradas_decoder\")\n","\n","  dec_salidas = tf.keras.layers.Embedding(tgt_vocab_tam, emb_dim)(dec_entradas)\n","  dec_salidas = PositionalEncoding(max_long, emb_dim, 0.1)(dec_salidas)\n","\n","  capas_decoder = []\n","  for _ in range(num_dec_capas):\n","    capas_decoder.append(keras_nlp.layers.TransformerDecoder(\n","           intermediate_dim=dim_mlp,\n","           num_heads=ncabezas,\n","           dropout=dropout,\n","           activation=\"relu\",\n","           name=None))\n","\n","  trf_salida = dec_salidas\n","\n","  for capa in capas_decoder:\n","       trf_salida = capa(decoder_sequence=trf_salida,\n","                         encoder_sequence=enc_salidas,\n","                         decoder_padding_mask=mask_entradas_decoder,\n","                         use_causal_mask=True)\n","\n","  for capa in capas_decoder:\n","       dec_salidas = capa(decoder_sequence=dec_salidas,\n","                          encoder_sequence=enc_seq_entradas,\n","                          decoder_padding_mask=mask_entradas_decoder,\n","                          use_causal_mask=True)\n","\n","  capa_salida = tf.keras.layers.Dense(tgt_vocab_tam,\n","                    activation=\"linear\")\n","\n","  salida_transformer = capa_salida(trf_salida)\n","  salida_decoder = capa_salida(dec_salidas)\n","\n","  # Definicion del Transformer\n","  encoder = tf.keras.Model([enc_entradas, mask_entradas_encoder],\n","                          enc_salidas,\n","                          name=\"encoder\",\n","  )\n","\n","  decoder = tf.keras.Model([dec_entradas, enc_seq_entradas, mask_entradas_decoder],\n","                          salida_decoder,\n","                          name=\"decoder\",\n","  )\n","\n","  transformer = tf.keras.Model([enc_entradas, dec_entradas, mask_entradas_encoder, mask_entradas_decoder],\n","                          salida_transformer,\n","                          name=\"transformer\",\n","  )\n","\n","  transformer.summary()\n","  perdida = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, ignore_class=0)\n","  optimizador = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","  transformer.compile(loss=perdida, optimizer=optimizador)\n","\n","  return transformer, encoder, decoder\n"]},{"cell_type":"markdown","source":["Instanciación del Transformer"],"metadata":{"id":"doOv6Liwfc4Q"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2YadKA4I24Ge","outputId":"97395ef9-aaf4-4ca7-e612-0393e204fd93"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"transformer\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," enc_entradas (InputLayer)   [(None, None)]               0         []                            \n","                                                                                                  \n"," embedding (Embedding)       (None, None, 512)            1484441   ['enc_entradas[0][0]']        \n","                                                          6                                       \n","                                                                                                  \n"," positional_encoding (Posit  (None, None, 512)            0         ['embedding[0][0]']           \n"," ionalEncoding)                                                                                   \n","                                                                                                  \n"," mask_entradas_encoder (Inp  [(None, None)]               0         []                            \n"," utLayer)                                                                                         \n","                                                                                                  \n"," transformer_encoder (Trans  (None, None, 512)            3152384   ['positional_encoding[0][0]', \n"," formerEncoder)                                                      'mask_entradas_encoder[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," transformer_encoder_1 (Tra  (None, None, 512)            3152384   ['transformer_encoder[0][0]', \n"," nsformerEncoder)                                                    'mask_entradas_encoder[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," transformer_encoder_2 (Tra  (None, None, 512)            3152384   ['transformer_encoder_1[0][0]'\n"," nsformerEncoder)                                                   , 'mask_entradas_encoder[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," dec_entradas (InputLayer)   [(None, None)]               0         []                            \n","                                                                                                  \n"," transformer_encoder_3 (Tra  (None, None, 512)            3152384   ['transformer_encoder_2[0][0]'\n"," nsformerEncoder)                                                   , 'mask_entradas_encoder[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," embedding_1 (Embedding)     (None, None, 512)            7566848   ['dec_entradas[0][0]']        \n","                                                                                                  \n"," transformer_encoder_4 (Tra  (None, None, 512)            3152384   ['transformer_encoder_3[0][0]'\n"," nsformerEncoder)                                                   , 'mask_entradas_encoder[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," positional_encoding_1 (Pos  (None, None, 512)            0         ['embedding_1[0][0]']         \n"," itionalEncoding)                                                                                 \n","                                                                                                  \n"," mask_entradas_decoder (Inp  [(None, None)]               0         []                            \n"," utLayer)                                                                                         \n","                                                                                                  \n"," transformer_encoder_5 (Tra  (None, None, 512)            3152384   ['transformer_encoder_4[0][0]'\n"," nsformerEncoder)                                                   , 'mask_entradas_encoder[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," transformer_decoder (Trans  (None, None, 512)            4204032   ['positional_encoding_1[0][0]'\n"," formerDecoder)                                                     , 'mask_entradas_decoder[0][0]\n","                                                                    ',                            \n","                                                                     'transformer_encoder_5[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," transformer_decoder_1 (Tra  (None, None, 512)            4204032   ['transformer_decoder[0][0]', \n"," nsformerDecoder)                                                    'mask_entradas_decoder[0][0]'\n","                                                                    , 'transformer_encoder_5[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," transformer_decoder_2 (Tra  (None, None, 512)            4204032   ['transformer_decoder_1[0][0]'\n"," nsformerDecoder)                                                   , 'mask_entradas_decoder[0][0]\n","                                                                    ',                            \n","                                                                     'transformer_encoder_5[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," transformer_decoder_3 (Tra  (None, None, 512)            4204032   ['transformer_decoder_2[0][0]'\n"," nsformerDecoder)                                                   , 'mask_entradas_decoder[0][0]\n","                                                                    ',                            \n","                                                                     'transformer_encoder_5[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," transformer_decoder_4 (Tra  (None, None, 512)            4204032   ['transformer_decoder_3[0][0]'\n"," nsformerDecoder)                                                   , 'mask_entradas_decoder[0][0]\n","                                                                    ',                            \n","                                                                     'transformer_encoder_5[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," transformer_decoder_5 (Tra  (None, None, 512)            4204032   ['transformer_decoder_4[0][0]'\n"," nsformerDecoder)                                                   , 'mask_entradas_decoder[0][0]\n","                                                                    ',                            \n","                                                                     'transformer_encoder_5[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," dense_12 (Dense)            (None, None, 14779)          7581627   ['transformer_decoder_5[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n","==================================================================================================\n","Total params: 74131387 (282.79 MB)\n","Trainable params: 74131387 (282.79 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}],"source":["max_long = max([len(x) for x in X + Y])\n","# Instancia del modelo Transformer\n","transformer, encoder, decoder = crear_transformer(\n","   max_long=max_long,\n","   emb_dim=512,\n","   num_enc_capas=6,\n","   num_dec_capas=6,\n","   ncabezas=8,\n","   src_vocab_tam=len(X_w2i),\n","   tgt_vocab_tam=len(Y_w2i),\n","   dim_mlp=2048,\n","   dropout=0.1\n",")"]},{"cell_type":"markdown","source":["Entrenamiento del modelo"],"metadata":{"id":"zp239MtBfhst"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ROdpKXNX-pAg","outputId":"7fb210c3-df5d-4d3d-d0df-36f72c3c9f6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","743/743 [==============================] - 350s 384ms/step - loss: 5.3182\n","Epoch 2/20\n","743/743 [==============================] - 265s 357ms/step - loss: 5.3015\n","Epoch 3/20\n","743/743 [==============================] - 263s 354ms/step - loss: 5.1645\n","Epoch 4/20\n","743/743 [==============================] - 264s 355ms/step - loss: 4.8761\n","Epoch 5/20\n","743/743 [==============================] - 263s 354ms/step - loss: 4.7273\n","Epoch 6/20\n","743/743 [==============================] - 263s 354ms/step - loss: 4.8893\n","Epoch 7/20\n","743/743 [==============================] - 264s 356ms/step - loss: 4.6304\n","Epoch 8/20\n","743/743 [==============================] - 264s 355ms/step - loss: 4.5480\n","Epoch 9/20\n","743/743 [==============================] - 264s 355ms/step - loss: 4.4777\n","Epoch 10/20\n","743/743 [==============================] - 263s 354ms/step - loss: 4.3213\n","Epoch 11/20\n","743/743 [==============================] - 263s 354ms/step - loss: 4.2240\n","Epoch 12/20\n","743/743 [==============================] - 262s 353ms/step - loss: 4.2508\n","Epoch 13/20\n","743/743 [==============================] - 264s 355ms/step - loss: 4.1603\n","Epoch 14/20\n","743/743 [==============================] - 263s 353ms/step - loss: 4.1272\n","Epoch 15/20\n","743/743 [==============================] - 262s 353ms/step - loss: 4.1369\n","Epoch 16/20\n","743/743 [==============================] - 262s 353ms/step - loss: 4.2835\n","Epoch 17/20\n","743/743 [==============================] - 262s 353ms/step - loss: 4.1684\n","Epoch 18/20\n","743/743 [==============================] - 262s 353ms/step - loss: 4.1235\n","Epoch 19/20\n","743/743 [==============================] - 262s 353ms/step - loss: 4.0705\n","Epoch 20/20\n","743/743 [==============================] - 261s 352ms/step - loss: 4.0213\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7e84d80af6a0>"]},"metadata":{},"execution_count":12}],"source":["epocas = 20\n","train_loader = generador_batch(X_train, Y_train, batch_size=128)\n","transformer.fit(train_loader, epochs=epocas, steps_per_epoch=len(X_train)//128, verbose=1)"]},{"cell_type":"markdown","source":["Definición de la función de decodificación"],"metadata":{"id":"_Izo8hJ-flRH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2182StKCF6Tw"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","\n","def decodificacion_voraz(codificador, decodificador, src, src_mask, max_len, tgt_w2i, tgt_i2w):\n","   # Codificación\n","   print(src.shape)\n","   print(src_mask.shape)\n","\n","   src_cod = codificador.predict([src, src_mask], verbose=0)\n","\n","   # Decodificación\n","   tgt_token = tf.constant([[tgt_w2i['SOS']]], dtype=tf.int64)\n","\n","   tgt_pred_decod = []\n","   for i in range(max_len):\n","       # Predicción del modelo\n","       tgt_pred = decodificador.predict([tgt_token, src_cod, (tgt_token == 0)], verbose=0)\n","       tgt_pred = tgt_pred[:, -1, :]  # Último token\n","\n","       # Nos quedamos con el token más probable\n","       tgt_pred = tf.argmax(tgt_pred, axis=-1).numpy()[0]\n","       tgt_pred_decod.append(tgt_i2w[tgt_pred])\n","\n","       print(f'token predicho: {tgt_pred}')\n","       print(f'secuencia: {tgt_pred_decod}')\n","\n","       # Preparamos la nueva entrada del decoder\n","       tgt_token = np.hstack((tgt_token, np.array([[tgt_pred]])))\n","\n","       # Comprobamos si se ha predicho el token de fin de secuencia\n","       if tgt_pred_decod[-1] == 'EOS':\n","           break\n","\n","   return tgt_pred_decod"]},{"cell_type":"markdown","source":["Función de traduccion"],"metadata":{"id":"qldCrPMSf7dE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"R29dnxS-HHzd"},"outputs":[],"source":["def traducir(codificador, decodificador, src_frase, src_w2i, tgt_w2i, tgt_i2w):\n","   # Codificamos la secuencia de entrada\n","   src_cod = codificar([src_frase], src_w2i)\n","   src_cod = tf.convert_to_tensor(src_cod, dtype=tf.int64)\n","   # src_cod = tf.expand_dims(src_cod, axis=0)  # Agregamos dimensión de batch [1, sec_len]\n","\n","   # Máscara de ceros para el source (dejamos ver todo)\n","   src_mask = tf.zeros((1, src_cod.shape[1]))\n","\n","   # Permitimos hasta 5 tokens más en la traducción\n","   max_len = src_cod.shape[1] + 5\n","\n","   # Iniciamos la traducción\n","   tgt_pred_decod = decodificacion_voraz(codificador, decodificador, src_cod, src_mask, max_len, tgt_w2i, tgt_i2w)\n","\n","   # Quitamos los tokens de inicio y fin de secuencia\n","   tgt_pred_decod = [t for t in tgt_pred_decod if t not in ['SOS', 'EOS']]\n","   return ' '.join(tgt_pred_decod)\n"]},{"cell_type":"markdown","source":["Prueba de traducción de una frase"],"metadata":{"id":"Rieu1aU1f9uS"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wjz8nMFuOkZs","outputId":"b49939f4-3fc3-4950-b755-18e35f9c1f38"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 11)\n","(1, 11)\n","token predicho: 4574\n","secuencia: ['He']\n","token predicho: 11534\n","secuencia: ['He', 'is']\n","token predicho: 2\n","secuencia: ['He', 'is', 'EOS']\n"]}],"source":["src_frase = 'Espero que te haya gustado el caso de estudio'\n","tgt_frase = traducir(\n","   encoder, decoder,\n","   src_frase,\n","   X_w2i,\n","   Y_w2i, Y_i2w,\n",")\n"]},{"cell_type":"markdown","source":["## Traducción con Hugging Face Transformers"],"metadata":{"id":"IhIS8J7dgB72"}},{"cell_type":"markdown","source":["Instalación de la librería de Hugging Face Transformers"],"metadata":{"id":"AEZ5p49jgY5f"}},{"cell_type":"code","source":["!pip install transformers\n","!pip install sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9YXLVHFcgI_o","executionInfo":{"status":"ok","timestamp":1691319937358,"user_tz":-120,"elapsed":16551,"user":{"displayName":"Antoniorv6","userId":"12791290898792286358"}},"outputId":"4f4b62b1-fdc6-410d-b784-8fcedcfd9d80"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n"]}]},{"cell_type":"markdown","source":["Creación del tokenizador y del modelo"],"metadata":{"id":"xHKBqVJ5gt1G"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, TFMarianMTModel\n","import tensorflow as tf\n","\n","tokenizador = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\")\n","modelo = TFMarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZN5ge6MhgTBF","executionInfo":{"status":"ok","timestamp":1691320528517,"user_tz":-120,"elapsed":6507,"user":{"displayName":"Antoniorv6","userId":"12791290898792286358"}},"outputId":"b1f31383-65e7-4e1e-fc9e-6e953c781436"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n","  warnings.warn(\"Recommended: pip install sacremoses.\")\n","All model checkpoint layers were used when initializing TFMarianMTModel.\n","\n","All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-fr-en.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"]}]},{"cell_type":"markdown","source":["Prueba del tokenizador implementado"],"metadata":{"id":"UxDjuAkzh-15"}},{"cell_type":"code","source":["tokenizador(\"Buenas tardes\", return_tensors=\"tf\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ov1iMxyTg1Xu","executionInfo":{"status":"ok","timestamp":1691320528518,"user_tz":-120,"elapsed":4,"user":{"displayName":"Antoniorv6","userId":"12791290898792286358"}},"outputId":"9e004bdd-7757-4701-a8cf-b51e1f3abf6a"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': <tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[7545, 8277,    9, 2065,  114,    0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1]], dtype=int32)>}"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["Prueba del modelo sin ajustarse"],"metadata":{"id":"XYPbW-rui0Ga"}},{"cell_type":"code","source":["entrada = tokenizador([\"Espero que te haya gustado el caso practico\"], return_tensors=\"tf\").input_ids\n","outputs = modelo.generate(entrada)\n","print(tokenizador.decode(outputs[0], skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SJlUSj6ni11C","executionInfo":{"status":"ok","timestamp":1691320531689,"user_tz":-120,"elapsed":3175,"user":{"displayName":"Antoniorv6","userId":"12791290898792286358"}},"outputId":"8dd82862-a512-40c3-9463-f0f2415feebf"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/tf_utils.py:854: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Espero que te haya gustado el caso practico\n"]}]},{"cell_type":"markdown","source":["División del conjunto de datos en entrenamiento y test con los datos sin tokenizar (80-20)"],"metadata":{"id":"RVNgl-DliHSo"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\\\n","                                                    test_size=0.2,\\\n","                                                    random_state=42)\n","print('¡Particiones realizadas!')\n","print(f'Tamaño del conjunto de entrenamiento: {len(X_train)}')\n","print(f'Tamaño del conjunto de test: {len(X_test)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HhBjdnkeiG1s","executionInfo":{"status":"ok","timestamp":1691320532078,"user_tz":-120,"elapsed":391,"user":{"displayName":"Antoniorv6","userId":"12791290898792286358"}},"outputId":"2b01c314-b3bc-4b91-f49d-f2300caff691"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["¡Particiones realizadas!\n","Tamaño del conjunto de entrenamiento: 95171\n","Tamaño del conjunto de test: 23793\n"]}]},{"cell_type":"markdown","source":["Reimplementación de la función de preprocesado de datos con el tokenizador implementado."],"metadata":{"id":"ubNVNhNFiP75"}},{"cell_type":"code","source":["def preproceso_batch(X, Y, tokenizador):\n","\n","   transformer_data = tokenizador(X, text_target=Y, return_tensors=\"tf\", padding=True)\n","\n","   return {\"input_ids\": transformer_data[\"input_ids\"], \"attention_mask\": transformer_data[\"attention_mask\"], \"labels\": transformer_data[\"labels\"]}"],"metadata":{"id":"UA8zW-MhiSc8","executionInfo":{"status":"ok","timestamp":1691320532079,"user_tz":-120,"elapsed":3,"user":{"displayName":"Antoniorv6","userId":"12791290898792286358"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["Reimplementación del generador de batches con el tokenizador implementado"],"metadata":{"id":"RXlajMdJieD0"}},{"cell_type":"code","source":["def generador_batch(X, Y, tokenizador, batch_size):\n","    idx = 0\n","    while True:\n","        bx = X[idx:idx+batch_size]\n","        by = Y[idx:idx+batch_size]\n","\n","        yield preproceso_batch(bx, by, tokenizador)\n","\n","        idx = (idx + batch_size) % len(X)\n","\n","train_loader = generador_batch(X_train, Y_train, tokenizador=tokenizador, batch_size=16)"],"metadata":{"id":"F5tyrR9MibhZ","executionInfo":{"status":"ok","timestamp":1691320532079,"user_tz":-120,"elapsed":3,"user":{"displayName":"Antoniorv6","userId":"12791290898792286358"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["Compilación del modelo para entrenarlo"],"metadata":{"id":"goSBgDhUio64"}},{"cell_type":"code","source":["modelo.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, ignore_class=0),\n","               optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.98, epsilon=1e-9))"],"metadata":{"id":"zgNbvFUvilP4","executionInfo":{"status":"ok","timestamp":1691320532079,"user_tz":-120,"elapsed":3,"user":{"displayName":"Antoniorv6","userId":"12791290898792286358"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["Ajuste del modelo para nuestro conjunto de datos"],"metadata":{"id":"Gzhozn42itmY"}},{"cell_type":"code","source":["modelo.fit(train_loader, epochs=1, steps_per_epoch=len(X_train)//16, verbose=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5p5NlUKxisvG","executionInfo":{"status":"ok","timestamp":1691321637652,"user_tz":-120,"elapsed":1105576,"user":{"displayName":"Antoniorv6","userId":"12791290898792286358"}},"outputId":"18a2fe96-e382-4880-a15e-26cca05685de"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["5948/5948 [==============================] - 1105s 168ms/step - loss: 0.6976\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7acf4694d9c0>"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["Prueba de traducción con el modelo ajustado"],"metadata":{"id":"eZaJA4fLkG9P"}},{"cell_type":"code","source":["entrada = tokenizador([\"Espero que te haya gustado el caso practico\"], return_tensors=\"tf\").input_ids\n","outputs = modelo.generate(entrada)\n","print(tokenizador.decode(outputs[0], skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QVmr4rL_j031","executionInfo":{"status":"ok","timestamp":1691321652229,"user_tz":-120,"elapsed":10304,"user":{"displayName":"Antoniorv6","userId":"12791290898792286358"}},"outputId":"736d5693-e68f-49a2-b6cb-ade831c8a751"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["I hope you'd enjoyed the practice case.\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}